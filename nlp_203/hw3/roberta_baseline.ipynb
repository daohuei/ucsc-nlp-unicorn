{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03bb114-a971-4063-a219-7a0467dc845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"deepset/roberta-base-squad2\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a9b91e4-9475-49d5-896e-241598d359c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Read COVID dataset\n",
    "\n",
    "def load_json_file_to_dict(file_name):\n",
    "    return json.load(open(file_name))\n",
    "data_dict = load_json_file_to_dict(\"data/covid-qa/covid-qa-dev.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e4dbb-b423-46c0-aad0-fbd6abbcbda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f24a6a8e-db73-45b3-8a47-9f15a29b8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(data_dict):\n",
    "    result_dict = {\n",
    "        'id':[], 'title':[], 'context':[], 'question':[], 'answers':[]\n",
    "    }\n",
    "    for article in data_dict['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa_pair in paragraph['qas']:\n",
    "                for ans in qa_pair[\"answers\"]:\n",
    "                    result_dict[\"answers\"].append({\n",
    "                        'answer_start': [ans[\"answer_start\"]],\n",
    "                        'text': [ans[\"text\"]]\n",
    "                    })\n",
    "                    result_dict[\"question\"].append(qa_pair[\"question\"])\n",
    "                    result_dict[\"context\"].append(paragraph[\"context\"])\n",
    "                    result_dict[\"title\"].append(paragraph[\"document_id\"])\n",
    "                    result_dict[\"id\"].append(qa_pair[\"id\"])\n",
    "\n",
    "    return result_dict.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7e01049-781b-40cd-8e4f-172732a5f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = Dataset.from_dict(reconstruct_data(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49472821-7111-474f-9a29-4409b99898ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 203\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "510c25cc-4441-40bf-b10c-a67b2aa644ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify this into covid dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dcd89ee-0b2c-485f-9cec-a9e63d97f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/Users/daohuei/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d50aa9f741f4386b1453abb82e5c9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# load the dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9441875b-8c5e-4e14-bc98-a86e6af6ab20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bdf750f-6ffc-4a09-99bd-d9250df2f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c540ecf-89af-4cd3-8cec-85d7e79b156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data preprocessing\n",
    "\n",
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24ac8fe-9e93-4af0-829b-a2c0dddc4021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-eb7c13e373f33819.arrow\n",
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-4e66715979a095a9.arrow\n"
     ]
    }
   ],
   "source": [
    "# data tokenization\n",
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac5b5ee-6718-4f14-81eb-805d86d0052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00589e5-4f15-4e52-ace1-07a282e4990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# get data collator\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c853747b-1632-443c-af1e-91499ce74f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9124317e-a864-4e0f-b772-e8f6de097e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set preprocessing\n",
    "def prepare_validation_features(examples):\n",
    "    \n",
    "    print(len(examples[\"question\"]))\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    print(len(examples[\"question\"]))\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    print(len(tokenized_examples[\"input_ids\"]))\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "        \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6f926676-2879-49ae-a56b-aafdc0c96c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73971d29da0f443dbf939b57f97cd1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "test_sample_set_covid = Dataset.from_dict(sample_dataset[:5])\n",
    "test_sample_set = Dataset.from_dict(datasets[\"validation\"][:5])\n",
    "\n",
    "# validation set tokenization\n",
    "validation_features = test_sample_set_covid.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=test_sample_set_covid.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "29929319-bbb6-4162-95f6-9e3ce2334060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'answer_start': [177, 177, 177],\n",
       "   'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
       "  {'answer_start': [249, 249, 249],\n",
       "   'text': ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers']},\n",
       "  {'answer_start': [403, 355, 355],\n",
       "   'text': ['Santa Clara, California',\n",
       "    \"Levi's Stadium\",\n",
       "    \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]},\n",
       "  {'answer_start': [177, 177, 177],\n",
       "   'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
       "  {'answer_start': [488, 488, 521], 'text': ['gold', 'gold', 'gold']}],\n",
       " [{'answer_start': [433], 'text': ['Gram positive, anaerobic bacterium']},\n",
       "  {'answer_start': [1992],\n",
       "   'text': ['adaptive strategy that enables bacteria to survive harsh environmental conditions for prolonged periods of time']},\n",
       "  {'answer_start': [2243], 'text': ['Spo0A']},\n",
       "  {'answer_start': [4755], 'text': ['toxins A and B']},\n",
       "  {'answer_start': [629],\n",
       "   'text': ['The capacities of nanopore sequencing for viral diagnostics']}])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = \"answers\"\n",
    "test_sample_set[key], test_sample_set_covid[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48474168-618b-42c6-8faa-a1ba3af17634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 5\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 5\n",
       " }))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_set_covid, test_sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10893a9c-5798-442c-86c6-b239c7b7a9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 227\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fe795bbf-7c7a-47cf-8fb7-37028a0bd92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 227\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "# inference on val set\n",
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651bccc-3618-4acb-aace-678b74b5d239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85fbb864-2793-4a08-8607-47973436816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[ 1.8830386 , -8.950881  , -9.310952  , ..., -9.187948  ,\n",
       "        -9.270747  , -9.673088  ],\n",
       "       [ 1.9403212 , -8.757857  , -9.298976  , ..., -4.2572184 ,\n",
       "        -0.08869352, -8.93426   ],\n",
       "       [ 2.7002494 , -8.733148  , -9.223859  , ..., -9.121831  ,\n",
       "        -7.9471684 , -9.290447  ],\n",
       "       ...,\n",
       "       [ 1.6147189 , -7.3919144 , -8.720754  , ..., -9.015832  ,\n",
       "        -7.604792  , -9.078127  ],\n",
       "       [ 2.7188263 , -8.298267  , -8.861992  , ..., -8.799892  ,\n",
       "        -8.272102  , -9.457897  ],\n",
       "       [ 2.0730934 , -8.020707  , -8.766091  , ..., -9.438511  ,\n",
       "        -9.438511  , -9.438511  ]], dtype=float32), array([[ 2.1263626, -8.772525 , -8.876476 , ..., -8.618605 , -6.676522 ,\n",
       "        -8.161184 ],\n",
       "       [ 2.522217 , -8.636038 , -8.86185  , ..., -7.5498385, -3.805926 ,\n",
       "        -9.091347 ],\n",
       "       [ 3.302677 , -8.502097 , -8.830866 , ..., -7.072866 , -8.367661 ,\n",
       "        -9.031145 ],\n",
       "       ...,\n",
       "       [ 2.0747316, -8.72905  , -8.9300785, ..., -5.48121  , -3.6246662,\n",
       "        -8.6798115],\n",
       "       [ 3.2380044, -9.058017 , -8.95061  , ..., -6.507587 , -5.8444395,\n",
       "        -8.409596 ],\n",
       "       [ 2.5666282, -8.92963  , -8.860515 , ..., -9.158847 , -9.158847 ,\n",
       "        -9.158847 ]], dtype=float32)), label_ids=None, metrics={'test_runtime': 40.5125, 'test_samples_per_second': 5.603, 'test_steps_per_second': 0.37})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4d8f2-27c9-4027-bf3a-98c73c06b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "491f191e-8ac0-4ceb-b1bc-b6ab4df107fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# postprocessing on predictions\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "deaab079-1bd5-43f1-90fc-2b0051b60b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 5 example predictions split into 227 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa184264a47149a8951b37e6939b88c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# postprocessing\n",
    "final_predictions = postprocess_qa_predictions(test_sample_set_covid, validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c6d1f99a-7f44-4eec-8e74-b2e58b78aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {k:v for k,v in final_predictions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c3a4403-3d39-404a-a67a-3c145dfb3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_gold(data_dict, pred_dict):\n",
    "    gold_dict = {\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    ids = pred_dict.keys()\n",
    "    \n",
    "    for article in data_dict['data']:\n",
    "        gold_article = {\"paragraphs\":[]}\n",
    "        \n",
    "        for paragraph in article['paragraphs']:\n",
    "            gold_paragraph = paragraph.copy()\n",
    "            gold_paragraph[\"qas\"] = []\n",
    "            for qa_pair in paragraph['qas']:\n",
    "                if qa_pair[\"id\"] in ids:\n",
    "                    gold_paragraph[\"qas\"].append(qa_pair)\n",
    "            gold_article[\"paragraphs\"].append(gold_paragraph)\n",
    "        gold_dict[\"data\"].append(gold_article)\n",
    "    return gold_dict\n",
    "gold_dict = create_sample_gold(data_dict, pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "22f44570-ce30-4ba2-a0ad-86af68b7dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to the file\n",
    "def dict_to_json(data_dict, file_name):\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        json.dump(data_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e14fbc21-ad7f-4fda-9317-d787ee7274ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_json(gold_dict, \"sample_gold.json\")\n",
    "dict_to_json(pred_dict, \"sample_pred.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "980e112c-87f0-4382-a7b4-2c040f81f182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f0cfc-e9ee-4d63-9e98-e3808cb5e09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cc362-4de6-4969-955c-d604688f8b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
