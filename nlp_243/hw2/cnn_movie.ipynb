{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ken/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(\"Using CUDA: {}\".format(cuda))\n",
    "\n",
    "device = torch.device(\"cpu\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, num_words):\n",
    "        self.data = \"train_data_merged_labels.csv\"\n",
    "        self.num_words = num_words\n",
    "        self.seq_len = 1\n",
    "        self.class_num = 1\n",
    "        self.vocabulary = None\n",
    "        self.label_types = None\n",
    "        self.label_idx_map = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        self.y = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        # Reads the raw csv file and split into\n",
    "        # sentences (x) and target (y)\n",
    "        df = pd.read_csv(self.data)\n",
    "        df = self.over_sampling(df)\n",
    "        # df.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "        df = df.rename(columns={\"utterances\": \"text\", \"Core Relations\": \"target\"})\n",
    "        self.x_raw = df[\"text\"].values\n",
    "        self.y = df[\"target\"].values\n",
    "        self.class_num = len(df[\"target\"].unique())\n",
    "\n",
    "    def clean_text(self):\n",
    "        # Removes special symbols and just keep\n",
    "        # words in lower or upper form\n",
    "        self.x_raw = [x.lower() for x in self.x_raw]\n",
    "        self.x_raw = [re.sub(r\"[^A-Za-z]+\", \" \", x) for x in self.x_raw]\n",
    "\n",
    "    def text_tokenization(self):\n",
    "        # Tokenizes each sentence by implementing the nltk tool\n",
    "        self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
    "        self.seq_len = max([len(tokens) for tokens in self.x_raw])\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary and keeps the \"x\" most frequent word\n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = idx + 1\n",
    "\n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary (vocabulary), it is transformed\n",
    "        # each token into its index based representatio\n",
    "        self.x_tokenized = list()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "\n",
    "    def label_to_idx(self):\n",
    "        self.label_types = np.unique(self.y)\n",
    "        self.label_idx_map = {label: idx for idx, label in enumerate(self.label_types)}\n",
    "        self.y = np.vectorize(lambda label: self.label_idx_map[label])(self.y)\n",
    "\n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required le\n",
    "        # it's padded with the index 0\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "            self.x_padded.append(sentence)\n",
    "\n",
    "        self.x_padded = np.array(self.x_padded)\n",
    "\n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.x_padded, self.y, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "    def over_sampling(self, train_df):\n",
    "      df_list = [train_df]\n",
    "      max_size = train_df[\"Core Relations\"].value_counts().max()\n",
    "      for class_index, group in train_df.groupby('Core Relations'):\n",
    "          df_list.append(group.sample(max_size-len(group), replace=True))\n",
    "      frame_new = pd.concat(df_list)\n",
    "      return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features, out_features, stride=2, num_words=2000, embedding_size=64\n",
    "    ):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        # Parameters regarding text preprocessing\n",
    "        self.seq_len = num_features\n",
    "        self.num_words = num_words\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # CNN parameters definition\n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        # Output size for each convolution\n",
    "        self.out_size = 32\n",
    "        # Number of strides for each convolution\n",
    "        self.stride = stride\n",
    "\n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.num_words + 1, self.embedding_size, padding_idx=0\n",
    "        )\n",
    "\n",
    "        # Convolution layers definition\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        # Max pooling layers definition\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        # Fully connected layer definition\n",
    "        self.fc = nn.Linear(self.in_features_fc(), out_features)\n",
    "\n",
    "    def in_features_fc(self):\n",
    "        \"\"\"Calculates the number of output features after Convolution + Max pooling\n",
    "\n",
    "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\n",
    "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        \"\"\"\n",
    "        # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "        out_conv_1 = (\n",
    "            (self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride\n",
    "        ) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "\n",
    "        # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "        out_conv_2 = (\n",
    "            (self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride\n",
    "        ) + 1\n",
    "        out_conv_2 = math.floor(out_conv_2)\n",
    "        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_2 = math.floor(out_pool_2)\n",
    "\n",
    "        # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "        out_conv_3 = (\n",
    "            (self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride\n",
    "        ) + 1\n",
    "        out_conv_3 = math.floor(out_conv_3)\n",
    "        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_3 = math.floor(out_pool_3)\n",
    "\n",
    "        # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "        out_conv_4 = (\n",
    "            (self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride\n",
    "        ) + 1\n",
    "        out_conv_4 = math.floor(out_conv_4)\n",
    "        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_4 = math.floor(out_pool_4)\n",
    "\n",
    "        # Returns \"flattened\" vector (input for fully connected layer)\n",
    "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "\n",
    "    def forward(self, x_in):\n",
    "\n",
    "        # Sequence of tokes is filterd through an embedding layer\n",
    "        x = self.embedding(x_in)\n",
    "        \n",
    "        # Convolution layer 1 is applied\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pool_1(x1)\n",
    "\n",
    "        # Convolution layer 2 is applied\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu((x2))\n",
    "        x2 = self.pool_2(x2)\n",
    "\n",
    "        # Convolution layer 3 is applied\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pool_3(x3)\n",
    "\n",
    "        # Convolution layer 4 is applied\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pool_4(x4)\n",
    "\n",
    "        # The output of each convolutional layer is concatenated into a unique vector\n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "    \n",
    "        # The \"flattened\" vector is passed through a fully connected layer\n",
    "        out = self.fc(union)\n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        # Activation function is applied\n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.softmax(out, dim=0)\n",
    "#         print(out)\n",
    "#         out = torch.softmax(out)\n",
    "        return out\n",
    "#         return out\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(clf, dataloader, optimizer, loss_func):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        # the training routine is these 5 steps:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "        y_pred = clf(x_in=X)\n",
    "        \n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, y.squeeze(0))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # ------------------------\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def train_models(clf, dataloader, dataset={}, hyper_params={}):\n",
    "    # get the hyperparameters\n",
    "    num_epochs = hyper_params.get(\"num_epochs\", 10000)\n",
    "    learning_rate = hyper_params.get(\"learning_rate\", 0.001)\n",
    "    batch_size = hyper_params.get(\"batch_size\", 1000)\n",
    "    optim_type = hyper_params.get(\"optim_type\", \"rms\")\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optim_type == \"adam\":\n",
    "        optimizer = optim.Adam(clf.parameters(), lr=learning_rate)\n",
    "    elif optim_type == \"rms\":\n",
    "        optimizer = optim.RMSprop(clf.parameters(), lr=learning_rate)\n",
    "    elif optim_type == \"sgd\":\n",
    "        optimizer = optim.Adam(clf.parameters(), lr=learning_rate)\n",
    "\n",
    "    losses = []\n",
    "    all_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_one_epoch(clf, dataloader, optimizer, loss_func)\n",
    "        print(f\"epoch:{epoch+1}, loss: {epoch_loss}\")\n",
    "        if dataset:\n",
    "            train_X, train_y = dataset[\"train\"]\n",
    "            val_X, val_y = dataset[\"val\"]\n",
    "            train_pred = torch.argmax(torch.softmax(clf(train_X), dim=0), dim=1)\n",
    "            val_pred = torch.argmax(torch.softmax(clf(val_X), dim=0), dim=1)\n",
    "            train_accu = accuracy(train_pred, train_y).item()\n",
    "            val_accu = accuracy(val_pred, val_y).item()\n",
    "            all_accuracy.append((train_accu, val_accu))\n",
    "            print(f\"train:{train_accu}, val: {val_accu}\")\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "    return losses, all_accuracy\n",
    "\n",
    "\n",
    "def validate_models(clf, dataloader, train_dataset, hyper_params={}):\n",
    "    losses, all_accuracy = train_models(clf, dataloader, train_dataset, hyper_params)\n",
    "\n",
    "    train_X, train_y = train_dataset[\"train\"]\n",
    "    val_X, val_y = train_dataset[\"val\"]\n",
    "    train_pred = torch.argmax(torch.softmax(clf(train_X), dim=0), dim=1)\n",
    "    val_pred = torch.argmax(torch.softmax(clf(val_X), dim=0), dim=1)\n",
    "\n",
    "    print(accuracy(train_pred, train_y).item())\n",
    "    print(accuracy(val_pred, val_y).item())\n",
    "    # print(classification_report(val_y.cpu().data.numpy(), val_pred.cpu().data.numpy()))\n",
    "\n",
    "    # a figure with 2x1 grid of Axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    # plt.ylabel('Cross Entropy Loss')\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"MLP Classification Performance\")\n",
    "    # plt.plot(losses, label=str(hyper_params))\n",
    "    ax.plot(\n",
    "        [train_accu for train_accu, _ in all_accuracy],\n",
    "        label=str(hyper_params) + \"_train\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        [val_accu for _, val_accu in all_accuracy], label=str(hyper_params) + \"_val\"\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def accuracy(pred, truth):\n",
    "    measurements = []\n",
    "    for p, t in zip(pred, truth):\n",
    "        measurements.append(p == t)\n",
    "    return torch.mean(torch.stack(measurements).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class UtteranceDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # Convert arrays to torch tensors\n",
    "        self.X = torch.tensor(x)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    # Must have\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    # Must have\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[12, 62, 54, ...,  0,  0,  0],\n",
       "        [34, 72, 29, ...,  0,  0,  0],\n",
       "        [ 8,  5, 25, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [30, 18, 15, ...,  0,  0,  0],\n",
       "        [16, 87,  2, ...,  0,  0,  0],\n",
       "        [30,  0,  0, ...,  0,  0,  0]]),\n",
       " (1577,),\n",
       " 21,\n",
       " 47)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Preprocessing(100)\n",
    "data.load_data()\n",
    "data.text_tokenization()\n",
    "data.build_vocabulary()\n",
    "data.word_to_idx()\n",
    "data.label_to_idx()\n",
    "data.padding_sentences()\n",
    "data.split_data()\n",
    "\n",
    "num_features = data.seq_len\n",
    "out_features = data.class_num\n",
    "\n",
    "data.x_train, data.y_train.shape, num_features, out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_dataset = UtteranceDataset(data.x_train, data.y_train)\n",
    "dataloader = DataLoader(dataset=utterance_dataset, \n",
    "                      batch_size=2000,\n",
    "                      shuffle=True)\n",
    "x_train = torch.tensor(data.x_train).to(device)\n",
    "y_train = torch.tensor(data.y_train).to(device)\n",
    "x_test = torch.tensor(data.x_test).to(device)\n",
    "y_test = torch.tensor(data.y_test).to(device)\n",
    "train_dataset = {\n",
    "    \"train\": (x_train, y_train),\n",
    "    \"val\": (x_test, y_test)\n",
    "}\n",
    "clf = CNNClassifier(num_features, out_features, stride=2, num_words=len(data.vocabulary)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss: 3.850146532058716\n",
      "train:0.06150919571518898, val: 0.060650888830423355\n",
      "epoch:2, loss: 3.8501205444335938\n",
      "train:0.059606850147247314, val: 0.07396449893712997\n",
      "epoch:3, loss: 3.850118398666382\n",
      "train:0.10336080938577652, val: 0.0710059180855751\n",
      "epoch:4, loss: 3.8500945568084717\n",
      "train:0.10653138905763626, val: 0.057692307978868484\n",
      "epoch:5, loss: 3.8500876426696777\n",
      "train:0.2149651199579239, val: 0.167159765958786\n",
      "epoch:6, loss: 3.8500468730926514\n",
      "train:0.16169942915439606, val: 0.12130177766084671\n",
      "epoch:7, loss: 3.8500475883483887\n",
      "train:0.19594165682792664, val: 0.19378697872161865\n",
      "epoch:8, loss: 3.85003924369812\n",
      "train:0.19657577574253082, val: 0.15680474042892456\n",
      "epoch:9, loss: 3.8500168323516846\n",
      "train:0.22954978048801422, val: 0.20562130212783813\n",
      "epoch:10, loss: 3.8500051498413086\n",
      "train:0.2517438232898712, val: 0.18491123616695404\n",
      "epoch:11, loss: 3.849980354309082\n",
      "train:0.2136968970298767, val: 0.19082839787006378\n",
      "epoch:12, loss: 3.8499906063079834\n",
      "train:0.3069118559360504, val: 0.24260355532169342\n",
      "epoch:13, loss: 3.8499553203582764\n",
      "train:0.317691832780838, val: 0.25\n",
      "epoch:14, loss: 3.849949598312378\n",
      "train:0.3024730384349823, val: 0.23076923191547394\n",
      "epoch:15, loss: 3.8499388694763184\n",
      "train:0.30564361810684204, val: 0.2529585659503937\n",
      "epoch:16, loss: 3.8499419689178467\n",
      "train:0.36525046825408936, val: 0.28106507658958435\n",
      "epoch:17, loss: 3.8499107360839844\n",
      "train:0.3392517566680908, val: 0.2869822382926941\n",
      "epoch:18, loss: 3.8499224185943604\n",
      "train:0.36525046825408936, val: 0.3121301829814911\n",
      "epoch:19, loss: 3.8499159812927246\n",
      "train:0.3969562351703644, val: 0.3505917191505432\n",
      "epoch:20, loss: 3.8498926162719727\n",
      "train:0.39885860681533813, val: 0.32248520851135254\n",
      "epoch:21, loss: 3.849879741668701\n",
      "train:0.40837031602859497, val: 0.3535502851009369\n",
      "epoch:22, loss: 3.8498756885528564\n",
      "train:0.4185161590576172, val: 0.334319531917572\n",
      "epoch:23, loss: 3.8498694896698\n",
      "train:0.41978439688682556, val: 0.3624260425567627\n",
      "epoch:24, loss: 3.849862575531006\n",
      "train:0.4280278980731964, val: 0.34763312339782715\n",
      "epoch:25, loss: 3.8498568534851074\n",
      "train:0.43627139925956726, val: 0.3757396340370178\n",
      "epoch:26, loss: 3.8498551845550537\n",
      "train:0.44134432077407837, val: 0.3505917191505432\n",
      "epoch:27, loss: 3.849856376647949\n",
      "train:0.4610019028186798, val: 0.3994082808494568\n",
      "epoch:28, loss: 3.8498401641845703\n",
      "train:0.4610019028186798, val: 0.3624260425567627\n",
      "epoch:29, loss: 3.8498268127441406\n",
      "train:0.4616360068321228, val: 0.3994082808494568\n",
      "epoch:30, loss: 3.8498294353485107\n",
      "train:0.47431832551956177, val: 0.3875739574432373\n",
      "epoch:31, loss: 3.8498191833496094\n",
      "train:0.4793912470340729, val: 0.415680468082428\n",
      "epoch:32, loss: 3.8498153686523438\n",
      "train:0.4616360068321228, val: 0.3949704170227051\n",
      "epoch:33, loss: 3.849810838699341\n",
      "train:0.471147745847702, val: 0.417159765958786\n",
      "epoch:34, loss: 3.8498146533966064\n",
      "train:0.49778059124946594, val: 0.4038461446762085\n",
      "epoch:35, loss: 3.8498055934906006\n",
      "train:0.4939759075641632, val: 0.4363905191421509\n",
      "epoch:36, loss: 3.849796772003174\n",
      "train:0.5123652219772339, val: 0.41124260425567627\n",
      "epoch:37, loss: 3.849794864654541\n",
      "train:0.49207356572151184, val: 0.44822484254837036\n",
      "epoch:38, loss: 3.8497979640960693\n",
      "train:0.5066581964492798, val: 0.4215976297855377\n",
      "epoch:39, loss: 3.8497862815856934\n",
      "train:0.5066581964492798, val: 0.4511834383010864\n",
      "epoch:40, loss: 3.8497817516326904\n",
      "train:0.48700064420700073, val: 0.4245562255382538\n",
      "epoch:41, loss: 3.84979248046875\n",
      "train:0.5206087231636047, val: 0.43934911489486694\n",
      "epoch:42, loss: 3.849792003631592\n",
      "train:0.4876347482204437, val: 0.3994082808494568\n",
      "epoch:43, loss: 3.8497836589813232\n",
      "train:0.5117311477661133, val: 0.4467455744743347\n",
      "epoch:44, loss: 3.849776029586792\n",
      "train:0.5225111246109009, val: 0.4378698170185089\n",
      "epoch:45, loss: 3.8497707843780518\n",
      "train:0.5149016976356506, val: 0.4378698170185089\n",
      "epoch:46, loss: 3.84977388381958\n",
      "train:0.5206087231636047, val: 0.4378698170185089\n",
      "epoch:47, loss: 3.8497660160064697\n",
      "train:0.5161699652671814, val: 0.45266273617744446\n",
      "epoch:48, loss: 3.8497703075408936\n",
      "train:0.5415345430374146, val: 0.4497041404247284\n",
      "epoch:49, loss: 3.8497676849365234\n",
      "train:0.5326569676399231, val: 0.4378698170185089\n",
      "epoch:50, loss: 3.8497631549835205\n",
      "train:0.5351933836936951, val: 0.4585798680782318\n",
      "epoch:51, loss: 3.8497536182403564\n",
      "train:0.5434368848800659, val: 0.41863906383514404\n",
      "epoch:52, loss: 3.84975528717041\n",
      "train:0.5688015222549438, val: 0.4822485148906708\n",
      "epoch:53, loss: 3.8497474193573\n",
      "train:0.5377298593521118, val: 0.44822484254837036\n",
      "epoch:54, loss: 3.8497531414031982\n",
      "train:0.5351933836936951, val: 0.4201183319091797\n",
      "epoch:55, loss: 3.8497495651245117\n",
      "train:0.5580215454101562, val: 0.440828412771225\n",
      "epoch:56, loss: 3.8497493267059326\n",
      "train:0.5453392267227173, val: 0.4215976297855377\n",
      "epoch:57, loss: 3.849745988845825\n",
      "train:0.5535827279090881, val: 0.4659763276576996\n",
      "epoch:58, loss: 3.8497469425201416\n",
      "train:0.5339251756668091, val: 0.4704141914844513\n",
      "epoch:59, loss: 3.8497354984283447\n",
      "train:0.5599238872528076, val: 0.45710060000419617\n",
      "epoch:60, loss: 3.8497447967529297\n",
      "train:0.5351933836936951, val: 0.43934911489486694\n",
      "epoch:61, loss: 3.849722146987915\n",
      "train:0.5396322011947632, val: 0.4822485148906708\n",
      "epoch:62, loss: 3.8497397899627686\n",
      "train:0.5548509955406189, val: 0.4452662765979767\n",
      "epoch:63, loss: 3.849731922149658\n",
      "train:0.5688015222549438, val: 0.45266273617744446\n",
      "epoch:64, loss: 3.8497495651245117\n",
      "train:0.5396322011947632, val: 0.4615384638309479\n",
      "epoch:65, loss: 3.849735975265503\n",
      "train:0.5624603629112244, val: 0.46005916595458984\n",
      "epoch:66, loss: 3.849729537963867\n",
      "train:0.5738744735717773, val: 0.45562130212783813\n",
      "epoch:67, loss: 3.849726676940918\n",
      "train:0.5637285709381104, val: 0.47337278723716736\n",
      "epoch:68, loss: 3.8497211933135986\n",
      "train:0.5580215454101562, val: 0.46005916595458984\n",
      "epoch:69, loss: 3.84971284866333\n",
      "train:0.5573874711990356, val: 0.4748520851135254\n",
      "epoch:70, loss: 3.8497209548950195\n",
      "train:0.5764108896255493, val: 0.45562130212783813\n",
      "epoch:71, loss: 3.849717378616333\n",
      "train:0.5637285709381104, val: 0.4718934893608093\n",
      "epoch:72, loss: 3.8497278690338135\n",
      "train:0.5700697302818298, val: 0.4718934893608093\n",
      "epoch:73, loss: 3.849724054336548\n",
      "train:0.5649968385696411, val: 0.47337278723716736\n",
      "epoch:74, loss: 3.8497138023376465\n",
      "train:0.5719720721244812, val: 0.46893492341041565\n",
      "epoch:75, loss: 3.8497066497802734\n",
      "train:0.5668991804122925, val: 0.4467455744743347\n",
      "epoch:76, loss: 3.849710702896118\n",
      "train:0.5808497071266174, val: 0.45562130212783813\n",
      "epoch:77, loss: 3.8497211933135986\n",
      "train:0.5764108896255493, val: 0.47781065106391907\n",
      "epoch:78, loss: 3.849714994430542\n",
      "train:0.5726062059402466, val: 0.48076921701431274\n",
      "epoch:79, loss: 3.849705696105957\n",
      "train:0.5757768154144287, val: 0.4541420042514801\n",
      "epoch:80, loss: 3.849700927734375\n",
      "train:0.5643627047538757, val: 0.47337278723716736\n",
      "epoch:81, loss: 3.8497071266174316\n",
      "train:0.5814838409423828, val: 0.46005916595458984\n",
      "epoch:82, loss: 3.849709987640381\n",
      "train:0.573240339756012, val: 0.4881656765937805\n",
      "epoch:83, loss: 3.849705457687378\n",
      "train:0.5865567326545715, val: 0.4615384638309479\n",
      "epoch:84, loss: 3.8497118949890137\n",
      "train:0.5814838409423828, val: 0.4866863787174225\n",
      "epoch:85, loss: 3.849708080291748\n",
      "train:0.5726062059402466, val: 0.4748520851135254\n",
      "epoch:86, loss: 3.8497154712677\n",
      "train:0.5884590744972229, val: 0.46005916595458984\n",
      "epoch:87, loss: 3.8496997356414795\n",
      "train:0.5694356560707092, val: 0.4704141914844513\n",
      "epoch:88, loss: 3.849703550338745\n",
      "train:0.6030437350273132, val: 0.4748520851135254\n",
      "epoch:89, loss: 3.849698543548584\n",
      "train:0.5795814990997314, val: 0.4615384638309479\n",
      "epoch:90, loss: 3.8496978282928467\n",
      "train:0.5859226584434509, val: 0.49852070212364197\n",
      "epoch:91, loss: 3.849700927734375\n",
      "train:0.5865567326545715, val: 0.48520711064338684\n",
      "epoch:92, loss: 3.849701404571533\n",
      "train:0.5719720721244812, val: 0.5\n",
      "epoch:93, loss: 3.8496925830841064\n",
      "train:0.5979708433151245, val: 0.4541420042514801\n",
      "epoch:94, loss: 3.8496925830841064\n",
      "train:0.5580215454101562, val: 0.4881656765937805\n",
      "epoch:95, loss: 3.849702835083008\n",
      "train:0.5871908664703369, val: 0.47337278723716736\n",
      "epoch:96, loss: 3.849702835083008\n",
      "train:0.5922638177871704, val: 0.4881656765937805\n",
      "epoch:97, loss: 3.8496973514556885\n",
      "train:0.6049460768699646, val: 0.47337278723716736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:98, loss: 3.849691390991211\n",
      "train:0.6024096608161926, val: 0.4926035404205322\n",
      "epoch:99, loss: 3.8496859073638916\n",
      "train:0.6125555038452148, val: 0.46893492341041565\n",
      "epoch:100, loss: 3.8496882915496826\n",
      "train:0.5935320258140564, val: 0.49408283829689026\n",
      "epoch:101, loss: 3.849684953689575\n",
      "train:0.5884590744972229, val: 0.48076921701431274\n",
      "epoch:102, loss: 3.849691152572632\n",
      "train:0.6074825525283813, val: 0.4911242723464966\n",
      "epoch:103, loss: 3.849694013595581\n",
      "train:0.5789473652839661, val: 0.4955621361732483\n",
      "epoch:104, loss: 3.84968638420105\n",
      "train:0.6074825525283813, val: 0.5162721872329712\n",
      "epoch:105, loss: 3.8496837615966797\n",
      "train:0.5960685014724731, val: 0.4674556255340576\n",
      "epoch:106, loss: 3.849684238433838\n",
      "train:0.5884590744972229, val: 0.4748520851135254\n",
      "epoch:107, loss: 3.8496787548065186\n",
      "train:0.5973367094993591, val: 0.48520711064338684\n",
      "epoch:108, loss: 3.8496854305267334\n",
      "train:0.6112872362136841, val: 0.46005916595458984\n",
      "epoch:109, loss: 3.849687099456787\n",
      "train:0.591629683971405, val: 0.49852070212364197\n",
      "epoch:110, loss: 3.849673271179199\n",
      "train:0.5973367094993591, val: 0.4748520851135254\n",
      "epoch:111, loss: 3.8496766090393066\n",
      "train:0.604312002658844, val: 0.48076921701431274\n",
      "epoch:112, loss: 3.849679946899414\n",
      "train:0.6036778688430786, val: 0.4615384638309479\n",
      "epoch:113, loss: 3.849675416946411\n",
      "train:0.6081166863441467, val: 0.4955621361732483\n",
      "epoch:114, loss: 3.8496809005737305\n",
      "train:0.6005073189735413, val: 0.4792899489402771\n",
      "epoch:115, loss: 3.8496806621551514\n",
      "train:0.606848418712616, val: 0.4926035404205322\n",
      "epoch:116, loss: 3.8496789932250977\n",
      "train:0.6081166863441467, val: 0.4659763276576996\n",
      "epoch:117, loss: 3.8496806621551514\n",
      "train:0.6163601875305176, val: 0.46005916595458984\n",
      "epoch:118, loss: 3.849679946899414\n",
      "train:0.5903614163398743, val: 0.4881656765937805\n",
      "epoch:119, loss: 3.8496792316436768\n",
      "train:0.6030437350273132, val: 0.4837278127670288\n",
      "epoch:120, loss: 3.8496692180633545\n",
      "train:0.6100190281867981, val: 0.48964497447013855\n",
      "epoch:121, loss: 3.849670886993408\n",
      "train:0.6112872362136841, val: 0.4911242723464966\n",
      "epoch:122, loss: 3.849674940109253\n",
      "train:0.5967025756835938, val: 0.47781065106391907\n",
      "epoch:123, loss: 3.8496809005737305\n",
      "train:0.606848418712616, val: 0.47633135318756104\n",
      "epoch:124, loss: 3.849674940109253\n",
      "train:0.6093848943710327, val: 0.48520711064338684\n",
      "epoch:125, loss: 3.849672317504883\n",
      "train:0.6112872362136841, val: 0.4881656765937805\n",
      "epoch:126, loss: 3.84967041015625\n",
      "train:0.6119213700294495, val: 0.5103550553321838\n",
      "epoch:127, loss: 3.8496594429016113\n",
      "train:0.6131895780563354, val: 0.48964497447013855\n",
      "epoch:128, loss: 3.8496601581573486\n",
      "train:0.6163601875305176, val: 0.4704141914844513\n",
      "epoch:129, loss: 3.8496663570404053\n",
      "train:0.6074825525283813, val: 0.46005916595458984\n",
      "epoch:130, loss: 3.849679946899414\n",
      "train:0.6074825525283813, val: 0.4704141914844513\n",
      "epoch:131, loss: 3.8496816158294678\n",
      "train:0.6258718967437744, val: 0.5177514553070068\n",
      "epoch:132, loss: 3.849672794342041\n",
      "train:0.5897273421287537, val: 0.4926035404205322\n",
      "epoch:133, loss: 3.849663019180298\n",
      "train:0.6087508201599121, val: 0.4881656765937805\n",
      "epoch:134, loss: 3.8496687412261963\n",
      "train:0.6119213700294495, val: 0.4866863787174225\n",
      "epoch:135, loss: 3.849670171737671\n",
      "train:0.6207990050315857, val: 0.4926035404205322\n",
      "epoch:136, loss: 3.849667549133301\n",
      "train:0.616994321346283, val: 0.4955621361732483\n",
      "epoch:137, loss: 3.8496551513671875\n",
      "train:0.6106531620025635, val: 0.48520711064338684\n",
      "epoch:138, loss: 3.8496687412261963\n",
      "train:0.6176283955574036, val: 0.49408283829689026\n",
      "epoch:139, loss: 3.8496639728546143\n",
      "train:0.5998731851577759, val: 0.45710060000419617\n",
      "epoch:140, loss: 3.8496592044830322\n",
      "train:0.6087508201599121, val: 0.4926035404205322\n",
      "epoch:141, loss: 3.8496646881103516\n",
      "train:0.6284083724021912, val: 0.5014792680740356\n",
      "epoch:142, loss: 3.849663019180298\n",
      "train:0.6214330792427063, val: 0.49852070212364197\n",
      "epoch:143, loss: 3.849665641784668\n",
      "train:0.5960685014724731, val: 0.47781065106391907\n",
      "epoch:144, loss: 3.8496673107147217\n",
      "train:0.6195307374000549, val: 0.5059171319007874\n",
      "epoch:145, loss: 3.8496506214141846\n",
      "train:0.6017755270004272, val: 0.47337278723716736\n",
      "epoch:146, loss: 3.8496625423431396\n",
      "train:0.6220672130584717, val: 0.4911242723464966\n",
      "epoch:147, loss: 3.8496625423431396\n",
      "train:0.6207990050315857, val: 0.4866863787174225\n",
      "epoch:148, loss: 3.8496599197387695\n",
      "train:0.6201648712158203, val: 0.4866863787174225\n",
      "epoch:149, loss: 3.849655866622925\n",
      "train:0.616994321346283, val: 0.5059171319007874\n",
      "epoch:150, loss: 3.849653720855713\n",
      "train:0.6138237118721008, val: 0.48964497447013855\n",
      "epoch:151, loss: 3.8496603965759277\n",
      "train:0.6265060305595398, val: 0.5103550553321838\n",
      "epoch:152, loss: 3.8496556282043457\n",
      "train:0.5973367094993591, val: 0.4837278127670288\n",
      "epoch:153, loss: 3.849663019180298\n",
      "train:0.6252378225326538, val: 0.5029585957527161\n",
      "epoch:154, loss: 3.8496623039245605\n",
      "train:0.6119213700294495, val: 0.4704141914844513\n",
      "epoch:155, loss: 3.849649429321289\n",
      "train:0.6093848943710327, val: 0.4911242723464966\n",
      "epoch:156, loss: 3.8496551513671875\n",
      "train:0.6207990050315857, val: 0.4837278127670288\n",
      "epoch:157, loss: 3.8496546745300293\n",
      "train:0.6163601875305176, val: 0.4866863787174225\n",
      "epoch:158, loss: 3.84965181350708\n",
      "train:0.6233354210853577, val: 0.5133135914802551\n",
      "epoch:159, loss: 3.8496594429016113\n",
      "train:0.6030437350273132, val: 0.4881656765937805\n",
      "epoch:160, loss: 3.8496577739715576\n",
      "train:0.6258718967437744, val: 0.49852070212364197\n",
      "epoch:161, loss: 3.8496620655059814\n",
      "train:0.6138237118721008, val: 0.4911242723464966\n",
      "epoch:162, loss: 3.849665641784668\n",
      "train:0.6296765804290771, val: 0.48964497447013855\n",
      "epoch:163, loss: 3.8496665954589844\n",
      "train:0.6328471899032593, val: 0.4822485148906708\n",
      "epoch:164, loss: 3.849654197692871\n",
      "train:0.6074825525283813, val: 0.4881656765937805\n",
      "epoch:165, loss: 3.849661111831665\n",
      "train:0.606848418712616, val: 0.49408283829689026\n",
      "epoch:166, loss: 3.849658489227295\n",
      "train:0.636651873588562, val: 0.5029585957527161\n",
      "epoch:167, loss: 3.849651575088501\n",
      "train:0.635383665561676, val: 0.4837278127670288\n",
      "epoch:168, loss: 3.849646806716919\n",
      "train:0.6258718967437744, val: 0.5014792680740356\n",
      "epoch:169, loss: 3.849653959274292\n",
      "train:0.6157260537147522, val: 0.4748520851135254\n",
      "epoch:170, loss: 3.8496437072753906\n",
      "train:0.6144578456878662, val: 0.5147929191589355\n",
      "epoch:171, loss: 3.849647045135498\n",
      "train:0.6246036887168884, val: 0.4866863787174225\n",
      "epoch:172, loss: 3.8496580123901367\n",
      "train:0.6296765804290771, val: 0.46449702978134155\n",
      "epoch:173, loss: 3.8496546745300293\n",
      "train:0.6322130560874939, val: 0.4748520851135254\n",
      "epoch:174, loss: 3.8496487140655518\n",
      "train:0.6328471899032593, val: 0.4955621361732483\n",
      "epoch:175, loss: 3.8496463298797607\n",
      "train:0.6157260537147522, val: 0.48076921701431274\n",
      "epoch:176, loss: 3.8496572971343994\n",
      "train:0.6328471899032593, val: 0.49408283829689026\n",
      "epoch:177, loss: 3.849646806716919\n",
      "train:0.6474318504333496, val: 0.48520711064338684\n",
      "epoch:178, loss: 3.849646806716919\n",
      "train:0.6112872362136841, val: 0.48964497447013855\n",
      "epoch:179, loss: 3.8496439456939697\n",
      "train:0.6176283955574036, val: 0.49408283829689026\n",
      "epoch:180, loss: 3.8496475219726562\n",
      "train:0.6290425062179565, val: 0.4748520851135254\n",
      "epoch:181, loss: 3.849642276763916\n",
      "train:0.6214330792427063, val: 0.48964497447013855\n",
      "epoch:182, loss: 3.8496406078338623\n",
      "train:0.6461635828018188, val: 0.47781065106391907\n",
      "epoch:183, loss: 3.8496551513671875\n",
      "train:0.6265060305595398, val: 0.49408283829689026\n",
      "epoch:184, loss: 3.8496508598327637\n",
      "train:0.6474318504333496, val: 0.47337278723716736\n",
      "epoch:185, loss: 3.8496413230895996\n",
      "train:0.6309448480606079, val: 0.5162721872329712\n",
      "epoch:186, loss: 3.849644422531128\n",
      "train:0.6499682664871216, val: 0.5118343234062195\n",
      "epoch:187, loss: 3.849641799926758\n",
      "train:0.635383665561676, val: 0.46893492341041565\n",
      "epoch:188, loss: 3.849642515182495\n",
      "train:0.6328471899032593, val: 0.4822485148906708\n",
      "epoch:189, loss: 3.849646806716919\n",
      "train:0.6093848943710327, val: 0.4970414340496063\n",
      "epoch:190, loss: 3.8496387004852295\n",
      "train:0.6334812641143799, val: 0.5\n",
      "epoch:191, loss: 3.8496484756469727\n",
      "train:0.6518706679344177, val: 0.5118343234062195\n",
      "epoch:192, loss: 3.849642515182495\n",
      "train:0.6398224234580994, val: 0.47781065106391907\n",
      "epoch:193, loss: 3.8496484756469727\n",
      "train:0.6322130560874939, val: 0.4866863787174225\n",
      "epoch:194, loss: 3.8496458530426025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:0.6442612409591675, val: 0.4837278127670288\n",
      "epoch:195, loss: 3.8496479988098145\n",
      "train:0.6410906910896301, val: 0.47781065106391907\n",
      "epoch:196, loss: 3.8496346473693848\n",
      "train:0.6410906910896301, val: 0.47337278723716736\n",
      "epoch:197, loss: 3.849640369415283\n",
      "train:0.6328471899032593, val: 0.4911242723464966\n",
      "epoch:198, loss: 3.849637508392334\n",
      "train:0.6417247653007507, val: 0.5133135914802551\n",
      "epoch:199, loss: 3.8496434688568115\n",
      "train:0.6277742385864258, val: 0.4970414340496063\n",
      "epoch:200, loss: 3.8496391773223877\n",
      "train:0.6290425062179565, val: 0.5029585957527161\n",
      "epoch:201, loss: 3.849637746810913\n",
      "train:0.6290425062179565, val: 0.48076921701431274\n",
      "epoch:202, loss: 3.84965181350708\n",
      "train:0.6322130560874939, val: 0.48520711064338684\n",
      "epoch:203, loss: 3.8496365547180176\n",
      "train:0.636651873588562, val: 0.5059171319007874\n",
      "epoch:204, loss: 3.8496391773223877\n",
      "train:0.6518706679344177, val: 0.4866863787174225\n",
      "epoch:205, loss: 3.8496413230895996\n",
      "train:0.6150919198989868, val: 0.4881656765937805\n",
      "epoch:206, loss: 3.849647045135498\n",
      "train:0.6315789222717285, val: 0.47633135318756104\n",
      "epoch:207, loss: 3.8496458530426025\n",
      "train:0.6207990050315857, val: 0.5118343234062195\n",
      "epoch:208, loss: 3.849648952484131\n",
      "train:0.6398224234580994, val: 0.5014792680740356\n",
      "epoch:209, loss: 3.8496437072753906\n",
      "train:0.623969554901123, val: 0.48520711064338684\n",
      "epoch:210, loss: 3.849648952484131\n",
      "train:0.6480659246444702, val: 0.4926035404205322\n",
      "epoch:211, loss: 3.8496272563934326\n",
      "train:0.6201648712158203, val: 0.48964497447013855\n",
      "epoch:212, loss: 3.849639654159546\n",
      "train:0.6341153979301453, val: 0.49852070212364197\n",
      "epoch:213, loss: 3.8496320247650146\n",
      "train:0.6474318504333496, val: 0.4955621361732483\n",
      "epoch:214, loss: 3.8496453762054443\n",
      "train:0.6423588991165161, val: 0.5014792680740356\n",
      "epoch:215, loss: 3.8496298789978027\n",
      "train:0.6296765804290771, val: 0.48964497447013855\n",
      "epoch:216, loss: 3.8496384620666504\n",
      "train:0.6537730097770691, val: 0.5147929191589355\n",
      "epoch:217, loss: 3.8496382236480713\n",
      "train:0.636651873588562, val: 0.4718934893608093\n",
      "epoch:218, loss: 3.849639415740967\n",
      "train:0.6372860074043274, val: 0.46893492341041565\n",
      "epoch:219, loss: 3.849637508392334\n",
      "train:0.6309448480606079, val: 0.48964497447013855\n",
      "epoch:220, loss: 3.8496320247650146\n",
      "train:0.6347495317459106, val: 0.47781065106391907\n",
      "epoch:221, loss: 3.8496506214141846\n",
      "train:0.6372860074043274, val: 0.4659763276576996\n",
      "epoch:222, loss: 3.849640369415283\n",
      "train:0.6074825525283813, val: 0.5029585957527161\n",
      "epoch:223, loss: 3.849637508392334\n",
      "train:0.637920081615448, val: 0.4866863787174225\n",
      "epoch:224, loss: 3.849637269973755\n",
      "train:0.6480659246444702, val: 0.48964497447013855\n",
      "epoch:225, loss: 3.8496391773223877\n",
      "train:0.6385542154312134, val: 0.5103550553321838\n",
      "epoch:226, loss: 3.8496346473693848\n",
      "train:0.6448953747749329, val: 0.5014792680740356\n",
      "epoch:227, loss: 3.849632978439331\n",
      "train:0.6594800353050232, val: 0.4911242723464966\n",
      "epoch:228, loss: 3.8496415615081787\n",
      "train:0.6423588991165161, val: 0.4674556255340576\n",
      "epoch:229, loss: 3.8496382236480713\n",
      "train:0.6265060305595398, val: 0.4822485148906708\n",
      "epoch:230, loss: 3.8496413230895996\n",
      "train:0.6455295085906982, val: 0.45710060000419617\n",
      "epoch:231, loss: 3.8496294021606445\n",
      "train:0.6613823771476746, val: 0.4837278127670288\n",
      "epoch:232, loss: 3.8496322631835938\n",
      "train:0.6582117676734924, val: 0.4926035404205322\n",
      "epoch:233, loss: 3.8496341705322266\n",
      "train:0.6499682664871216, val: 0.5044378638267517\n",
      "epoch:234, loss: 3.8496320247650146\n",
      "train:0.650602400302887, val: 0.4881656765937805\n",
      "epoch:235, loss: 3.8496334552764893\n",
      "train:0.6417247653007507, val: 0.5221893787384033\n",
      "epoch:236, loss: 3.849637746810913\n",
      "train:0.6360177397727966, val: 0.49408283829689026\n",
      "epoch:237, loss: 3.8496291637420654\n",
      "train:0.6334812641143799, val: 0.5044378638267517\n",
      "epoch:238, loss: 3.8496406078338623\n",
      "train:0.6404565572738647, val: 0.4955621361732483\n",
      "epoch:239, loss: 3.849630355834961\n",
      "train:0.6474318504333496, val: 0.49408283829689026\n",
      "epoch:240, loss: 3.8496346473693848\n",
      "train:0.6518706679344177, val: 0.4881656765937805\n",
      "epoch:241, loss: 3.849635601043701\n",
      "train:0.6467977166175842, val: 0.4837278127670288\n",
      "epoch:242, loss: 3.8496286869049072\n",
      "train:0.6391883492469788, val: 0.5177514553070068\n",
      "epoch:243, loss: 3.8496298789978027\n",
      "train:0.6531388759613037, val: 0.4970414340496063\n",
      "epoch:244, loss: 3.849630117416382\n",
      "train:0.6474318504333496, val: 0.4955621361732483\n",
      "epoch:245, loss: 3.849630117416382\n",
      "train:0.6499682664871216, val: 0.523668646812439\n",
      "epoch:246, loss: 3.849635601043701\n",
      "train:0.6480659246444702, val: 0.4926035404205322\n",
      "epoch:247, loss: 3.849642276763916\n",
      "train:0.6309448480606079, val: 0.5162721872329712\n",
      "epoch:248, loss: 3.8496339321136475\n",
      "train:0.6512365341186523, val: 0.4718934893608093\n",
      "epoch:249, loss: 3.84962797164917\n",
      "train:0.6214330792427063, val: 0.47337278723716736\n",
      "epoch:250, loss: 3.8496546745300293\n",
      "train:0.6328471899032593, val: 0.5\n",
      "epoch:251, loss: 3.8496556282043457\n",
      "train:0.6341153979301453, val: 0.5133135914802551\n",
      "epoch:252, loss: 3.8496317863464355\n",
      "train:0.6480659246444702, val: 0.5147929191589355\n",
      "epoch:253, loss: 3.8496408462524414\n",
      "train:0.6594800353050232, val: 0.5103550553321838\n",
      "epoch:254, loss: 3.8496320247650146\n",
      "train:0.6417247653007507, val: 0.4748520851135254\n",
      "epoch:255, loss: 3.8496294021606445\n",
      "train:0.6518706679344177, val: 0.5059171319007874\n",
      "epoch:256, loss: 3.849635124206543\n",
      "train:0.6303107142448425, val: 0.47337278723716736\n",
      "epoch:257, loss: 3.8496339321136475\n",
      "train:0.637920081615448, val: 0.48076921701431274\n",
      "epoch:258, loss: 3.8496313095092773\n",
      "train:0.6322130560874939, val: 0.4955621361732483\n",
      "epoch:259, loss: 3.849635362625122\n",
      "train:0.6303107142448425, val: 0.4970414340496063\n",
      "epoch:260, loss: 3.8496298789978027\n",
      "train:0.6391883492469788, val: 0.5073964595794678\n",
      "epoch:261, loss: 3.849632501602173\n",
      "train:0.6455295085906982, val: 0.4955621361732483\n",
      "epoch:262, loss: 3.8496241569519043\n",
      "train:0.6474318504333496, val: 0.4881656765937805\n",
      "epoch:263, loss: 3.8496294021606445\n",
      "train:0.6531388759613037, val: 0.49408283829689026\n",
      "epoch:264, loss: 3.849627733230591\n",
      "train:0.6429930329322815, val: 0.47633135318756104\n",
      "epoch:265, loss: 3.849625825881958\n",
      "train:0.6474318504333496, val: 0.4955621361732483\n",
      "epoch:266, loss: 3.8496289253234863\n",
      "train:0.6569435596466064, val: 0.5029585957527161\n",
      "epoch:267, loss: 3.8496346473693848\n",
      "train:0.6563094258308411, val: 0.4881656765937805\n",
      "epoch:268, loss: 3.849625825881958\n",
      "train:0.6417247653007507, val: 0.45266273617744446\n",
      "epoch:269, loss: 3.849635601043701\n",
      "train:0.667723536491394, val: 0.4955621361732483\n",
      "epoch:270, loss: 3.8496298789978027\n",
      "train:0.6398224234580994, val: 0.49852070212364197\n",
      "epoch:271, loss: 3.849626064300537\n",
      "train:0.6404565572738647, val: 0.4822485148906708\n",
      "epoch:272, loss: 3.8496251106262207\n",
      "train:0.6455295085906982, val: 0.49408283829689026\n",
      "epoch:273, loss: 3.8496220111846924\n",
      "train:0.6341153979301453, val: 0.5\n",
      "epoch:274, loss: 3.8496270179748535\n",
      "train:0.6588459014892578, val: 0.5103550553321838\n",
      "epoch:275, loss: 3.8496241569519043\n",
      "train:0.6487000584602356, val: 0.5\n",
      "epoch:276, loss: 3.849623680114746\n",
      "train:0.6569435596466064, val: 0.5029585957527161\n",
      "epoch:277, loss: 3.8496251106262207\n",
      "train:0.6727964282035828, val: 0.4866863787174225\n",
      "epoch:278, loss: 3.8496406078338623\n",
      "train:0.6341153979301453, val: 0.4630177617073059\n",
      "epoch:279, loss: 3.849648952484131\n",
      "train:0.6651870608329773, val: 0.5088757276535034\n",
      "epoch:280, loss: 3.849642276763916\n",
      "train:0.6544070839881897, val: 0.4955621361732483\n",
      "epoch:281, loss: 3.8496248722076416\n",
      "train:0.6563094258308411, val: 0.49852070212364197\n",
      "epoch:282, loss: 3.8496286869049072\n",
      "train:0.6645529270172119, val: 0.49852070212364197\n",
      "epoch:283, loss: 3.8496217727661133\n",
      "train:0.6639188528060913, val: 0.49852070212364197\n",
      "epoch:284, loss: 3.8496108055114746\n",
      "train:0.6474318504333496, val: 0.5147929191589355\n",
      "epoch:285, loss: 3.8496251106262207\n",
      "train:0.6708940863609314, val: 0.4674556255340576\n",
      "epoch:286, loss: 3.84962797164917\n",
      "train:0.6512365341186523, val: 0.5014792680740356\n",
      "epoch:287, loss: 3.849618911743164\n",
      "train:0.6613823771476746, val: 0.47781065106391907\n",
      "epoch:288, loss: 3.8496286869049072\n",
      "train:0.6455295085906982, val: 0.4866863787174225\n",
      "epoch:289, loss: 3.8496322631835938\n",
      "train:0.6607482433319092, val: 0.4748520851135254\n",
      "epoch:290, loss: 3.8496322631835938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:0.6391883492469788, val: 0.5014792680740356\n",
      "epoch:291, loss: 3.8496344089508057\n",
      "train:0.6410906910896301, val: 0.4955621361732483\n",
      "epoch:292, loss: 3.849621057510376\n",
      "train:0.6436271667480469, val: 0.4970414340496063\n",
      "epoch:293, loss: 3.849630355834961\n",
      "train:0.6385542154312134, val: 0.5044378638267517\n",
      "epoch:294, loss: 3.849630832672119\n",
      "train:0.6487000584602356, val: 0.5103550553321838\n",
      "epoch:295, loss: 3.8496227264404297\n",
      "train:0.6588459014892578, val: 0.4748520851135254\n",
      "epoch:296, loss: 3.849626064300537\n",
      "train:0.6512365341186523, val: 0.4926035404205322\n",
      "epoch:297, loss: 3.8496227264404297\n",
      "train:0.6626505851745605, val: 0.4911242723464966\n",
      "epoch:298, loss: 3.8496267795562744\n",
      "train:0.6569435596466064, val: 0.4881656765937805\n",
      "epoch:299, loss: 3.8496313095092773\n",
      "train:0.6455295085906982, val: 0.49408283829689026\n",
      "epoch:300, loss: 3.8496193885803223\n",
      "train:0.6391883492469788, val: 0.4704141914844513\n",
      "epoch:301, loss: 3.849618911743164\n",
      "train:0.6455295085906982, val: 0.47337278723716736\n",
      "epoch:302, loss: 3.849623918533325\n",
      "train:0.6645529270172119, val: 0.5103550553321838\n",
      "epoch:303, loss: 3.849627733230591\n",
      "train:0.6607482433319092, val: 0.4926035404205322\n",
      "epoch:304, loss: 3.8496317863464355\n",
      "train:0.6569435596466064, val: 0.4955621361732483\n",
      "epoch:305, loss: 3.8496174812316895\n",
      "train:0.6341153979301453, val: 0.4837278127670288\n",
      "epoch:306, loss: 3.849626064300537\n",
      "train:0.6601141691207886, val: 0.5103550553321838\n",
      "epoch:307, loss: 3.8496272563934326\n",
      "train:0.6480659246444702, val: 0.5044378638267517\n",
      "epoch:308, loss: 3.8496205806732178\n",
      "train:0.6727964282035828, val: 0.47633135318756104\n",
      "epoch:309, loss: 3.8496291637420654\n",
      "train:0.6461635828018188, val: 0.5088757276535034\n",
      "epoch:310, loss: 3.8496310710906982\n",
      "train:0.649334192276001, val: 0.48076921701431274\n",
      "epoch:311, loss: 3.8496317863464355\n",
      "train:0.6372860074043274, val: 0.5118343234062195\n",
      "epoch:312, loss: 3.849625825881958\n",
      "train:0.6448953747749329, val: 0.4674556255340576\n",
      "epoch:313, loss: 3.849630117416382\n",
      "train:0.6455295085906982, val: 0.526627242565155\n",
      "epoch:314, loss: 3.849625587463379\n",
      "train:0.6607482433319092, val: 0.4704141914844513\n",
      "epoch:315, loss: 3.849623918533325\n",
      "train:0.6455295085906982, val: 0.48076921701431274\n",
      "epoch:316, loss: 3.8496341705322266\n",
      "train:0.6372860074043274, val: 0.4837278127670288\n",
      "epoch:317, loss: 3.8496294021606445\n",
      "train:0.6461635828018188, val: 0.5118343234062195\n",
      "epoch:318, loss: 3.8496205806732178\n",
      "train:0.6658211946487427, val: 0.49852070212364197\n",
      "epoch:319, loss: 3.8496246337890625\n",
      "train:0.6391883492469788, val: 0.4911242723464966\n",
      "epoch:320, loss: 3.8496294021606445\n",
      "train:0.6391883492469788, val: 0.4748520851135254\n",
      "epoch:321, loss: 3.849626064300537\n",
      "train:0.6664552688598633, val: 0.48964497447013855\n",
      "epoch:322, loss: 3.8496248722076416\n",
      "train:0.6474318504333496, val: 0.5147929191589355\n",
      "epoch:323, loss: 3.8496224880218506\n",
      "train:0.6499682664871216, val: 0.4615384638309479\n",
      "epoch:324, loss: 3.849621057510376\n",
      "train:0.6645529270172119, val: 0.4970414340496063\n",
      "epoch:325, loss: 3.8496227264404297\n",
      "train:0.6537730097770691, val: 0.47633135318756104\n",
      "epoch:326, loss: 3.8496227264404297\n",
      "train:0.6575776934623718, val: 0.46449702978134155\n",
      "epoch:327, loss: 3.849621534347534\n",
      "train:0.6487000584602356, val: 0.48520711064338684\n",
      "epoch:328, loss: 3.849620819091797\n",
      "train:0.6347495317459106, val: 0.48520711064338684\n",
      "epoch:329, loss: 3.849626064300537\n",
      "train:0.6613823771476746, val: 0.5162721872329712\n",
      "epoch:330, loss: 3.8496241569519043\n",
      "train:0.6702600121498108, val: 0.47337278723716736\n",
      "epoch:331, loss: 3.8496217727661133\n",
      "train:0.6563094258308411, val: 0.5014792680740356\n",
      "epoch:332, loss: 3.849621057510376\n",
      "train:0.6645529270172119, val: 0.4911242723464966\n",
      "epoch:333, loss: 3.8496220111846924\n",
      "train:0.6525047421455383, val: 0.46893492341041565\n",
      "epoch:334, loss: 3.849625825881958\n",
      "train:0.6461635828018188, val: 0.49408283829689026\n",
      "epoch:335, loss: 3.8496227264404297\n",
      "train:0.6531388759613037, val: 0.4792899489402771\n",
      "epoch:336, loss: 3.849623203277588\n",
      "train:0.6607482433319092, val: 0.5029585957527161\n",
      "epoch:337, loss: 3.849618434906006\n",
      "train:0.6525047421455383, val: 0.4926035404205322\n",
      "epoch:338, loss: 3.8496179580688477\n",
      "train:0.6658211946487427, val: 0.5059171319007874\n",
      "epoch:339, loss: 3.8496291637420654\n",
      "train:0.637920081615448, val: 0.5073964595794678\n",
      "epoch:340, loss: 3.849627733230591\n",
      "train:0.6582117676734924, val: 0.47633135318756104\n",
      "epoch:341, loss: 3.8496198654174805\n",
      "train:0.6550412178039551, val: 0.4911242723464966\n",
      "epoch:342, loss: 3.8496201038360596\n",
      "train:0.6670894026756287, val: 0.5118343234062195\n",
      "epoch:343, loss: 3.8496217727661133\n",
      "train:0.6429930329322815, val: 0.4822485148906708\n",
      "epoch:344, loss: 3.8496084213256836\n",
      "train:0.6575776934623718, val: 0.5103550553321838\n",
      "epoch:345, loss: 3.849627733230591\n",
      "train:0.6303107142448425, val: 0.45562130212783813\n",
      "epoch:346, loss: 3.8496289253234863\n",
      "train:0.6461635828018188, val: 0.5207100510597229\n",
      "epoch:347, loss: 3.849637746810913\n",
      "train:0.6423588991165161, val: 0.4837278127670288\n",
      "epoch:348, loss: 3.8496246337890625\n",
      "train:0.6594800353050232, val: 0.48076921701431274\n",
      "epoch:349, loss: 3.8496131896972656\n",
      "train:0.6410906910896301, val: 0.49408283829689026\n",
      "epoch:350, loss: 3.8496170043945312\n",
      "train:0.6563094258308411, val: 0.4718934893608093\n",
      "epoch:351, loss: 3.8496217727661133\n",
      "train:0.6556753516197205, val: 0.47781065106391907\n",
      "epoch:352, loss: 3.8496193885803223\n",
      "train:0.6315789222717285, val: 0.4837278127670288\n",
      "epoch:353, loss: 3.8496289253234863\n",
      "train:0.6785035133361816, val: 0.4837278127670288\n",
      "epoch:354, loss: 3.8496198654174805\n",
      "train:0.6664552688598633, val: 0.4511834383010864\n",
      "epoch:355, loss: 3.849622964859009\n",
      "train:0.6429930329322815, val: 0.5177514553070068\n",
      "epoch:356, loss: 3.849611520767212\n",
      "train:0.667723536491394, val: 0.4837278127670288\n",
      "epoch:357, loss: 3.8496124744415283\n",
      "train:0.6772352457046509, val: 0.47633135318756104\n",
      "epoch:358, loss: 3.8496196269989014\n",
      "train:0.6601141691207886, val: 0.5029585957527161\n",
      "epoch:359, loss: 3.8496108055114746\n",
      "train:0.6499682664871216, val: 0.5\n",
      "epoch:360, loss: 3.849623203277588\n",
      "train:0.6442612409591675, val: 0.4881656765937805\n",
      "epoch:361, loss: 3.8496193885803223\n",
      "train:0.6601141691207886, val: 0.5177514553070068\n",
      "epoch:362, loss: 3.849614381790161\n",
      "train:0.6664552688598633, val: 0.5014792680740356\n",
      "epoch:363, loss: 3.849625825881958\n",
      "train:0.6480659246444702, val: 0.4955621361732483\n",
      "epoch:364, loss: 3.849611759185791\n",
      "train:0.6588459014892578, val: 0.5133135914802551\n",
      "epoch:365, loss: 3.8496158123016357\n",
      "train:0.6455295085906982, val: 0.5014792680740356\n",
      "epoch:366, loss: 3.8496131896972656\n",
      "train:0.6391883492469788, val: 0.5\n",
      "epoch:367, loss: 3.8496170043945312\n",
      "train:0.6639188528060913, val: 0.5147929191589355\n",
      "epoch:368, loss: 3.849616289138794\n",
      "train:0.6410906910896301, val: 0.5133135914802551\n",
      "epoch:369, loss: 3.849621534347534\n",
      "train:0.6480659246444702, val: 0.4970414340496063\n",
      "epoch:370, loss: 3.8496294021606445\n",
      "train:0.6563094258308411, val: 0.49408283829689026\n",
      "epoch:371, loss: 3.8496146202087402\n",
      "train:0.6436271667480469, val: 0.5014792680740356\n",
      "epoch:372, loss: 3.849612236022949\n",
      "train:0.6563094258308411, val: 0.48520711064338684\n",
      "epoch:373, loss: 3.8496174812316895\n",
      "train:0.6487000584602356, val: 0.4704141914844513\n",
      "epoch:374, loss: 3.8496193885803223\n",
      "train:0.6537730097770691, val: 0.4926035404205322\n",
      "epoch:375, loss: 3.8496220111846924\n",
      "train:0.6708940863609314, val: 0.4881656765937805\n",
      "epoch:376, loss: 3.8496201038360596\n",
      "train:0.6480659246444702, val: 0.47633135318756104\n",
      "epoch:377, loss: 3.8496055603027344\n",
      "train:0.6512365341186523, val: 0.5029585957527161\n",
      "epoch:378, loss: 3.8496155738830566\n",
      "train:0.6626505851745605, val: 0.5059171319007874\n",
      "epoch:379, loss: 3.849618911743164\n",
      "train:0.6303107142448425, val: 0.4926035404205322\n",
      "epoch:380, loss: 3.8496170043945312\n",
      "train:0.6544070839881897, val: 0.5\n",
      "epoch:381, loss: 3.8496196269989014\n",
      "train:0.6715282201766968, val: 0.5103550553321838\n",
      "epoch:382, loss: 3.8496146202087402\n",
      "train:0.6410906910896301, val: 0.5207100510597229\n",
      "epoch:383, loss: 3.849620819091797\n",
      "train:0.6436271667480469, val: 0.5147929191589355\n",
      "epoch:384, loss: 3.849613904953003\n",
      "train:0.667723536491394, val: 0.4881656765937805\n",
      "epoch:385, loss: 3.8496146202087402\n",
      "train:0.6607482433319092, val: 0.5088757276535034\n",
      "epoch:386, loss: 3.8496196269989014\n",
      "train:0.6550412178039551, val: 0.46005916595458984\n",
      "epoch:387, loss: 3.8496217727661133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:0.6563094258308411, val: 0.4792899489402771\n",
      "epoch:388, loss: 3.8496196269989014\n",
      "train:0.6518706679344177, val: 0.5177514553070068\n",
      "epoch:389, loss: 3.8496174812316895\n",
      "train:0.6372860074043274, val: 0.5073964595794678\n",
      "epoch:390, loss: 3.849613666534424\n",
      "train:0.6867470145225525, val: 0.5059171319007874\n",
      "epoch:391, loss: 3.8496220111846924\n",
      "train:0.6518706679344177, val: 0.4866863787174225\n",
      "epoch:392, loss: 3.8496193885803223\n",
      "train:0.6588459014892578, val: 0.48076921701431274\n",
      "epoch:393, loss: 3.849613666534424\n",
      "train:0.6582117676734924, val: 0.4866863787174225\n",
      "epoch:394, loss: 3.8496177196502686\n",
      "train:0.6550412178039551, val: 0.49408283829689026\n",
      "epoch:395, loss: 3.8496177196502686\n",
      "train:0.6537730097770691, val: 0.49852070212364197\n",
      "epoch:396, loss: 3.8496272563934326\n",
      "train:0.6575776934623718, val: 0.5118343234062195\n",
      "epoch:397, loss: 3.8496105670928955\n",
      "train:0.6753329038619995, val: 0.4926035404205322\n",
      "epoch:398, loss: 3.8496241569519043\n",
      "train:0.6702600121498108, val: 0.5133135914802551\n",
      "epoch:399, loss: 3.8496193885803223\n",
      "train:0.6360177397727966, val: 0.47633135318756104\n",
      "epoch:400, loss: 3.8496179580688477\n",
      "train:0.6645529270172119, val: 0.4792899489402771\n",
      "epoch:401, loss: 3.8496055603027344\n",
      "train:0.6683576703071594, val: 0.5014792680740356\n",
      "epoch:402, loss: 3.849621534347534\n",
      "train:0.6582117676734924, val: 0.5059171319007874\n",
      "epoch:403, loss: 3.8496053218841553\n",
      "train:0.6582117676734924, val: 0.4866863787174225\n",
      "epoch:404, loss: 3.8496134281158447\n",
      "train:0.6620165109634399, val: 0.48076921701431274\n",
      "epoch:405, loss: 3.849623203277588\n",
      "train:0.6398224234580994, val: 0.49408283829689026\n",
      "epoch:406, loss: 3.8496148586273193\n",
      "train:0.6670894026756287, val: 0.4792899489402771\n",
      "epoch:407, loss: 3.849609613418579\n",
      "train:0.6544070839881897, val: 0.5118343234062195\n",
      "epoch:408, loss: 3.849613666534424\n",
      "train:0.6563094258308411, val: 0.4955621361732483\n",
      "epoch:409, loss: 3.8496198654174805\n",
      "train:0.6683576703071594, val: 0.48520711064338684\n",
      "epoch:410, loss: 3.849611520767212\n",
      "train:0.6518706679344177, val: 0.5029585957527161\n",
      "epoch:411, loss: 3.8496131896972656\n",
      "train:0.6556753516197205, val: 0.5029585957527161\n",
      "epoch:412, loss: 3.849611282348633\n",
      "train:0.6277742385864258, val: 0.5044378638267517\n",
      "epoch:413, loss: 3.8496177196502686\n",
      "train:0.6360177397727966, val: 0.4926035404205322\n",
      "epoch:414, loss: 3.8496103286743164\n",
      "train:0.6455295085906982, val: 0.4837278127670288\n",
      "epoch:415, loss: 3.8496153354644775\n",
      "train:0.6518706679344177, val: 0.5073964595794678\n",
      "epoch:416, loss: 3.849621534347534\n",
      "train:0.649334192276001, val: 0.5207100510597229\n",
      "epoch:417, loss: 3.8496127128601074\n",
      "train:0.6512365341186523, val: 0.5044378638267517\n",
      "epoch:418, loss: 3.8496103286743164\n",
      "train:0.6544070839881897, val: 0.47781065106391907\n",
      "epoch:419, loss: 3.849618911743164\n",
      "train:0.6410906910896301, val: 0.5\n",
      "epoch:420, loss: 3.8496105670928955\n",
      "train:0.6499682664871216, val: 0.47633135318756104\n",
      "epoch:421, loss: 3.8496193885803223\n",
      "train:0.6277742385864258, val: 0.5059171319007874\n",
      "epoch:422, loss: 3.8496179580688477\n",
      "train:0.6696258783340454, val: 0.4881656765937805\n",
      "epoch:423, loss: 3.8496246337890625\n",
      "train:0.6512365341186523, val: 0.4866863787174225\n",
      "epoch:424, loss: 3.8496174812316895\n",
      "train:0.6664552688598633, val: 0.5014792680740356\n",
      "epoch:425, loss: 3.8496103286743164\n",
      "train:0.6461635828018188, val: 0.4837278127670288\n",
      "epoch:426, loss: 3.8496146202087402\n",
      "train:0.6531388759613037, val: 0.4748520851135254\n",
      "epoch:427, loss: 3.8496103286743164\n",
      "train:0.6658211946487427, val: 0.5029585957527161\n",
      "epoch:428, loss: 3.8496198654174805\n",
      "train:0.6670894026756287, val: 0.5118343234062195\n",
      "epoch:429, loss: 3.8496248722076416\n",
      "train:0.6588459014892578, val: 0.5044378638267517\n",
      "epoch:430, loss: 3.8496170043945312\n",
      "train:0.6487000584602356, val: 0.48964497447013855\n",
      "epoch:431, loss: 3.849609136581421\n",
      "train:0.6537730097770691, val: 0.4837278127670288\n",
      "epoch:432, loss: 3.849612236022949\n",
      "train:0.6734305620193481, val: 0.5221893787384033\n",
      "epoch:433, loss: 3.849620819091797\n",
      "train:0.6157260537147522, val: 0.48964497447013855\n",
      "epoch:434, loss: 3.849626064300537\n",
      "train:0.6639188528060913, val: 0.47633135318756104\n",
      "epoch:435, loss: 3.849618434906006\n",
      "train:0.6594800353050232, val: 0.5133135914802551\n",
      "epoch:436, loss: 3.8496131896972656\n",
      "train:0.6594800353050232, val: 0.4866863787174225\n",
      "epoch:437, loss: 3.849609613418579\n",
      "train:0.6391883492469788, val: 0.5088757276535034\n",
      "epoch:438, loss: 3.849609613418579\n",
      "train:0.6436271667480469, val: 0.48520711064338684\n",
      "epoch:439, loss: 3.8496217727661133\n",
      "train:0.6632847189903259, val: 0.523668646812439\n",
      "epoch:440, loss: 3.849611520767212\n",
      "train:0.6645529270172119, val: 0.5\n",
      "epoch:441, loss: 3.849606990814209\n",
      "train:0.6467977166175842, val: 0.4837278127670288\n",
      "epoch:442, loss: 3.849607229232788\n",
      "train:0.6467977166175842, val: 0.49408283829689026\n",
      "epoch:443, loss: 3.8496081829071045\n",
      "train:0.6620165109634399, val: 0.4970414340496063\n",
      "epoch:444, loss: 3.849614381790161\n",
      "train:0.6480659246444702, val: 0.4822485148906708\n",
      "epoch:445, loss: 3.849626064300537\n",
      "train:0.6588459014892578, val: 0.5103550553321838\n",
      "epoch:446, loss: 3.8496227264404297\n",
      "train:0.649334192276001, val: 0.48520711064338684\n",
      "epoch:447, loss: 3.8496248722076416\n",
      "train:0.6563094258308411, val: 0.4837278127670288\n",
      "epoch:448, loss: 3.849613666534424\n",
      "train:0.6575776934623718, val: 0.5177514553070068\n",
      "epoch:449, loss: 3.8496224880218506\n",
      "train:0.6664552688598633, val: 0.4926035404205322\n",
      "epoch:450, loss: 3.8496158123016357\n",
      "train:0.6531388759613037, val: 0.5073964595794678\n",
      "epoch:451, loss: 3.849628210067749\n",
      "train:0.6487000584602356, val: 0.4970414340496063\n",
      "epoch:452, loss: 3.8496131896972656\n",
      "train:0.6556753516197205, val: 0.5\n",
      "epoch:453, loss: 3.8496079444885254\n",
      "train:0.6455295085906982, val: 0.48076921701431274\n",
      "epoch:454, loss: 3.8496153354644775\n",
      "train:0.6334812641143799, val: 0.5118343234062195\n",
      "epoch:455, loss: 3.849613904953003\n",
      "train:0.6518706679344177, val: 0.5059171319007874\n",
      "epoch:456, loss: 3.849611759185791\n",
      "train:0.6487000584602356, val: 0.48964497447013855\n",
      "epoch:457, loss: 3.8496100902557373\n",
      "train:0.6607482433319092, val: 0.4911242723464966\n",
      "epoch:458, loss: 3.8496108055114746\n",
      "train:0.649334192276001, val: 0.4970414340496063\n",
      "epoch:459, loss: 3.8496086597442627\n",
      "train:0.6620165109634399, val: 0.5044378638267517\n",
      "epoch:460, loss: 3.8496153354644775\n",
      "train:0.6487000584602356, val: 0.49408283829689026\n",
      "epoch:461, loss: 3.8496148586273193\n",
      "train:0.6550412178039551, val: 0.5059171319007874\n",
      "epoch:462, loss: 3.8496086597442627\n",
      "train:0.6544070839881897, val: 0.49852070212364197\n",
      "epoch:463, loss: 3.849611759185791\n",
      "train:0.6582117676734924, val: 0.5014792680740356\n",
      "epoch:464, loss: 3.849613904953003\n",
      "train:0.6550412178039551, val: 0.5014792680740356\n",
      "epoch:465, loss: 3.849616527557373\n",
      "train:0.6544070839881897, val: 0.5088757276535034\n",
      "epoch:466, loss: 3.849618673324585\n",
      "train:0.6759670376777649, val: 0.49852070212364197\n",
      "epoch:467, loss: 3.8496158123016357\n",
      "train:0.6525047421455383, val: 0.4748520851135254\n",
      "epoch:468, loss: 3.849613904953003\n",
      "train:0.6740646958351135, val: 0.47781065106391907\n",
      "epoch:469, loss: 3.8496179580688477\n",
      "train:0.6582117676734924, val: 0.4970414340496063\n",
      "epoch:470, loss: 3.8496224880218506\n",
      "train:0.6467977166175842, val: 0.49408283829689026\n",
      "epoch:471, loss: 3.8496179580688477\n",
      "train:0.6670894026756287, val: 0.47781065106391907\n",
      "epoch:472, loss: 3.8496148586273193\n",
      "train:0.6778693795204163, val: 0.5088757276535034\n",
      "epoch:473, loss: 3.8496127128601074\n",
      "train:0.6556753516197205, val: 0.5088757276535034\n",
      "epoch:474, loss: 3.8496193885803223\n",
      "train:0.6569435596466064, val: 0.47781065106391907\n",
      "epoch:475, loss: 3.849620819091797\n",
      "train:0.6499682664871216, val: 0.4881656765937805\n",
      "epoch:476, loss: 3.8496146202087402\n",
      "train:0.66899174451828, val: 0.49852070212364197\n",
      "epoch:477, loss: 3.8496158123016357\n",
      "train:0.6448953747749329, val: 0.5073964595794678\n",
      "epoch:478, loss: 3.8496134281158447\n",
      "train:0.6734305620193481, val: 0.5207100510597229\n",
      "epoch:479, loss: 3.849613666534424\n",
      "train:0.6455295085906982, val: 0.5\n",
      "epoch:480, loss: 3.849607229232788\n",
      "train:0.6499682664871216, val: 0.48520711064338684\n",
      "epoch:481, loss: 3.8496201038360596\n",
      "train:0.6442612409591675, val: 0.4881656765937805\n",
      "epoch:482, loss: 3.849616527557373\n",
      "train:0.6563094258308411, val: 0.5133135914802551\n",
      "epoch:483, loss: 3.8496148586273193\n",
      "train:0.6448953747749329, val: 0.523668646812439\n",
      "epoch:484, loss: 3.849616765975952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:0.649334192276001, val: 0.49408283829689026\n",
      "epoch:485, loss: 3.8496153354644775\n",
      "train:0.6531388759613037, val: 0.5\n",
      "epoch:486, loss: 3.8496084213256836\n",
      "train:0.6594800353050232, val: 0.4792899489402771\n",
      "epoch:487, loss: 3.8496131896972656\n",
      "train:0.6531388759613037, val: 0.48964497447013855\n",
      "epoch:488, loss: 3.8496084213256836\n",
      "train:0.649334192276001, val: 0.4911242723464966\n",
      "epoch:489, loss: 3.8496177196502686\n",
      "train:0.6594800353050232, val: 0.4911242723464966\n",
      "epoch:490, loss: 3.8496060371398926\n",
      "train:0.6696258783340454, val: 0.4792899489402771\n",
      "epoch:491, loss: 3.8496146202087402\n",
      "train:0.6429930329322815, val: 0.4970414340496063\n",
      "epoch:492, loss: 3.8496086597442627\n",
      "train:0.6474318504333496, val: 0.5221893787384033\n",
      "epoch:493, loss: 3.849609136581421\n",
      "train:0.6594800353050232, val: 0.4837278127670288\n",
      "epoch:494, loss: 3.8496124744415283\n",
      "train:0.6582117676734924, val: 0.5044378638267517\n",
      "epoch:495, loss: 3.8496227264404297\n",
      "train:0.6518706679344177, val: 0.48520711064338684\n",
      "epoch:496, loss: 3.849607467651367\n",
      "train:0.649334192276001, val: 0.4837278127670288\n",
      "epoch:497, loss: 3.849616765975952\n",
      "train:0.6594800353050232, val: 0.5251479148864746\n",
      "epoch:498, loss: 3.849613666534424\n",
      "train:0.6626505851745605, val: 0.5014792680740356\n",
      "epoch:499, loss: 3.849606990814209\n",
      "train:0.6639188528060913, val: 0.5162721872329712\n",
      "epoch:500, loss: 3.849621534347534\n",
      "train:0.6544070839881897, val: 0.4866863787174225\n",
      "epoch:501, loss: 3.8496193885803223\n",
      "train:0.5611921548843384, val: 0.4748520851135254\n",
      "epoch:502, loss: 3.8496551513671875\n",
      "train:0.6607482433319092, val: 0.5133135914802551\n",
      "epoch:503, loss: 3.849616289138794\n",
      "train:0.6525047421455383, val: 0.46893492341041565\n",
      "epoch:504, loss: 3.849628210067749\n",
      "train:0.6550412178039551, val: 0.5192307829856873\n",
      "epoch:505, loss: 3.8496387004852295\n",
      "train:0.6664552688598633, val: 0.5088757276535034\n",
      "epoch:506, loss: 3.849612236022949\n",
      "train:0.649334192276001, val: 0.4837278127670288\n",
      "epoch:507, loss: 3.8496124744415283\n",
      "train:0.6303107142448425, val: 0.4955621361732483\n",
      "epoch:508, loss: 3.849616289138794\n",
      "train:0.635383665561676, val: 0.5088757276535034\n",
      "epoch:509, loss: 3.8496146202087402\n",
      "train:0.6854787468910217, val: 0.4822485148906708\n",
      "epoch:510, loss: 3.8496148586273193\n",
      "train:0.6518706679344177, val: 0.45710060000419617\n",
      "epoch:511, loss: 3.849620819091797\n",
      "train:0.6715282201766968, val: 0.5207100510597229\n",
      "epoch:512, loss: 3.849611282348633\n",
      "train:0.6455295085906982, val: 0.5059171319007874\n",
      "epoch:513, loss: 3.8496127128601074\n",
      "train:0.6639188528060913, val: 0.4718934893608093\n",
      "epoch:514, loss: 3.849604368209839\n",
      "train:0.6436271667480469, val: 0.5177514553070068\n",
      "epoch:515, loss: 3.8496084213256836\n",
      "train:0.6398224234580994, val: 0.48964497447013855\n",
      "epoch:516, loss: 3.8496086597442627\n",
      "train:0.6537730097770691, val: 0.49852070212364197\n",
      "epoch:517, loss: 3.8496158123016357\n",
      "train:0.6550412178039551, val: 0.5103550553321838\n",
      "epoch:518, loss: 3.8496079444885254\n",
      "train:0.6632847189903259, val: 0.5162721872329712\n",
      "epoch:519, loss: 3.849609613418579\n",
      "train:0.6537730097770691, val: 0.4792899489402771\n",
      "epoch:520, loss: 3.8496177196502686\n",
      "train:0.6620165109634399, val: 0.5310651063919067\n",
      "epoch:521, loss: 3.849609375\n",
      "train:0.6702600121498108, val: 0.5133135914802551\n",
      "epoch:522, loss: 3.849613666534424\n",
      "train:0.6467977166175842, val: 0.4837278127670288\n",
      "epoch:523, loss: 3.849611759185791\n",
      "train:0.66899174451828, val: 0.5\n",
      "epoch:524, loss: 3.8496079444885254\n",
      "train:0.649334192276001, val: 0.4718934893608093\n",
      "epoch:525, loss: 3.8496155738830566\n",
      "train:0.6759670376777649, val: 0.4704141914844513\n",
      "epoch:526, loss: 3.8496100902557373\n",
      "train:0.6607482433319092, val: 0.5059171319007874\n",
      "epoch:527, loss: 3.849612236022949\n",
      "train:0.6315789222717285, val: 0.4822485148906708\n",
      "epoch:528, loss: 3.8496170043945312\n",
      "train:0.650602400302887, val: 0.4955621361732483\n",
      "epoch:529, loss: 3.849626302719116\n",
      "train:0.6544070839881897, val: 0.5029585957527161\n",
      "epoch:530, loss: 3.8496062755584717\n",
      "train:0.6391883492469788, val: 0.48964497447013855\n",
      "epoch:531, loss: 3.8496227264404297\n",
      "train:0.6531388759613037, val: 0.5073964595794678\n",
      "epoch:532, loss: 3.8496153354644775\n",
      "train:0.6487000584602356, val: 0.5251479148864746\n",
      "epoch:533, loss: 3.849609136581421\n",
      "train:0.6518706679344177, val: 0.4970414340496063\n",
      "epoch:534, loss: 3.8496108055114746\n",
      "train:0.6575776934623718, val: 0.4911242723464966\n",
      "epoch:535, loss: 3.8496100902557373\n",
      "train:0.6537730097770691, val: 0.49852070212364197\n",
      "epoch:536, loss: 3.8496170043945312\n",
      "train:0.6258718967437744, val: 0.5\n",
      "epoch:537, loss: 3.849607229232788\n",
      "train:0.6556753516197205, val: 0.4970414340496063\n",
      "epoch:538, loss: 3.849620819091797\n",
      "train:0.6544070839881897, val: 0.4970414340496063\n",
      "epoch:539, loss: 3.8496105670928955\n",
      "train:0.6575776934623718, val: 0.4837278127670288\n",
      "epoch:540, loss: 3.8496174812316895\n",
      "train:0.6601141691207886, val: 0.5059171319007874\n",
      "epoch:541, loss: 3.8496100902557373\n",
      "train:0.6417247653007507, val: 0.5147929191589355\n",
      "epoch:542, loss: 3.849611282348633\n",
      "train:0.6563094258308411, val: 0.5088757276535034\n",
      "epoch:543, loss: 3.8496062755584717\n",
      "train:0.6417247653007507, val: 0.4911242723464966\n",
      "epoch:544, loss: 3.8496134281158447\n",
      "train:0.6410906910896301, val: 0.4911242723464966\n",
      "epoch:545, loss: 3.8496105670928955\n",
      "train:0.6708940863609314, val: 0.523668646812439\n",
      "epoch:546, loss: 3.8496170043945312\n",
      "train:0.6721623539924622, val: 0.4792899489402771\n",
      "epoch:547, loss: 3.849609136581421\n",
      "train:0.6499682664871216, val: 0.4911242723464966\n",
      "epoch:548, loss: 3.8496100902557373\n",
      "train:0.6746987700462341, val: 0.5118343234062195\n",
      "epoch:549, loss: 3.8496146202087402\n",
      "train:0.6696258783340454, val: 0.5059171319007874\n",
      "epoch:550, loss: 3.849611282348633\n",
      "train:0.6398224234580994, val: 0.4911242723464966\n",
      "epoch:551, loss: 3.8496053218841553\n",
      "train:0.6461635828018188, val: 0.47781065106391907\n",
      "epoch:552, loss: 3.8496158123016357\n",
      "train:0.6436271667480469, val: 0.5014792680740356\n",
      "epoch:553, loss: 3.849606990814209\n",
      "train:0.6575776934623718, val: 0.5014792680740356\n",
      "epoch:554, loss: 3.8496103286743164\n",
      "train:0.6683576703071594, val: 0.4792899489402771\n",
      "epoch:555, loss: 3.849612236022949\n",
      "train:0.6487000584602356, val: 0.4866863787174225\n",
      "epoch:556, loss: 3.8496158123016357\n",
      "train:0.6480659246444702, val: 0.5162721872329712\n",
      "epoch:557, loss: 3.8496170043945312\n",
      "train:0.6455295085906982, val: 0.5162721872329712\n",
      "epoch:558, loss: 3.849613666534424\n",
      "train:0.6607482433319092, val: 0.49408283829689026\n",
      "epoch:559, loss: 3.8496081829071045\n",
      "train:0.667723536491394, val: 0.48076921701431274\n",
      "epoch:560, loss: 3.8496081829071045\n",
      "train:0.6569435596466064, val: 0.5044378638267517\n",
      "epoch:561, loss: 3.849613666534424\n",
      "train:0.6588459014892578, val: 0.4970414340496063\n",
      "epoch:562, loss: 3.849621534347534\n",
      "train:0.6487000584602356, val: 0.49408283829689026\n",
      "epoch:563, loss: 3.849611759185791\n",
      "train:0.6664552688598633, val: 0.5192307829856873\n",
      "epoch:564, loss: 3.849607229232788\n",
      "train:0.6512365341186523, val: 0.4911242723464966\n",
      "epoch:565, loss: 3.849602222442627\n",
      "train:0.6594800353050232, val: 0.5088757276535034\n",
      "epoch:566, loss: 3.8496086597442627\n",
      "train:0.6467977166175842, val: 0.5014792680740356\n",
      "epoch:567, loss: 3.849616289138794\n",
      "train:0.6645529270172119, val: 0.5014792680740356\n",
      "epoch:568, loss: 3.849611520767212\n",
      "train:0.6550412178039551, val: 0.5044378638267517\n",
      "epoch:569, loss: 3.849623918533325\n",
      "train:0.6328471899032593, val: 0.5014792680740356\n",
      "epoch:570, loss: 3.849611282348633\n",
      "train:0.6582117676734924, val: 0.5044378638267517\n",
      "epoch:571, loss: 3.8496017456054688\n",
      "train:0.6556753516197205, val: 0.4881656765937805\n",
      "epoch:572, loss: 3.849607229232788\n",
      "train:0.649334192276001, val: 0.5073964595794678\n",
      "epoch:573, loss: 3.849609136581421\n",
      "train:0.6588459014892578, val: 0.4866863787174225\n",
      "epoch:574, loss: 3.8496053218841553\n",
      "train:0.6575776934623718, val: 0.48076921701431274\n",
      "epoch:575, loss: 3.8496131896972656\n",
      "train:0.6512365341186523, val: 0.4659763276576996\n",
      "epoch:576, loss: 3.8496081829071045\n",
      "train:0.6436271667480469, val: 0.4881656765937805\n",
      "epoch:577, loss: 3.849609136581421\n",
      "train:0.6436271667480469, val: 0.48520711064338684\n",
      "epoch:578, loss: 3.8496007919311523\n",
      "train:0.6461635828018188, val: 0.5029585957527161\n",
      "epoch:579, loss: 3.8496060371398926\n",
      "train:0.6556753516197205, val: 0.4955621361732483\n",
      "epoch:580, loss: 3.8496127128601074\n",
      "train:0.6436271667480469, val: 0.4881656765937805\n",
      "epoch:581, loss: 3.8496127128601074\n",
      "train:0.6721623539924622, val: 0.49408283829689026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:582, loss: 3.8496100902557373\n",
      "train:0.6613823771476746, val: 0.4881656765937805\n",
      "epoch:583, loss: 3.849604368209839\n",
      "train:0.6391883492469788, val: 0.45266273617744446\n",
      "epoch:584, loss: 3.849613666534424\n",
      "train:0.6594800353050232, val: 0.5029585957527161\n",
      "epoch:585, loss: 3.8496105670928955\n",
      "train:0.6626505851745605, val: 0.5103550553321838\n",
      "epoch:586, loss: 3.8495981693267822\n",
      "train:0.6525047421455383, val: 0.5118343234062195\n",
      "epoch:587, loss: 3.8496177196502686\n",
      "train:0.6626505851745605, val: 0.5029585957527161\n",
      "epoch:588, loss: 3.849604368209839\n",
      "train:0.6429930329322815, val: 0.4926035404205322\n",
      "epoch:589, loss: 3.849616527557373\n",
      "train:0.6588459014892578, val: 0.5118343234062195\n",
      "epoch:590, loss: 3.8496081829071045\n",
      "train:0.6746987700462341, val: 0.4866863787174225\n",
      "epoch:591, loss: 3.8496127128601074\n",
      "train:0.6601141691207886, val: 0.49852070212364197\n",
      "epoch:592, loss: 3.849616527557373\n",
      "train:0.6436271667480469, val: 0.48076921701431274\n",
      "epoch:593, loss: 3.849621057510376\n",
      "train:0.6442612409591675, val: 0.5044378638267517\n",
      "epoch:594, loss: 3.849602460861206\n",
      "train:0.6575776934623718, val: 0.4970414340496063\n",
      "epoch:595, loss: 3.849604368209839\n",
      "train:0.6404565572738647, val: 0.4970414340496063\n",
      "epoch:596, loss: 3.8496079444885254\n",
      "train:0.6474318504333496, val: 0.4970414340496063\n",
      "epoch:597, loss: 3.849609375\n",
      "train:0.6601141691207886, val: 0.5088757276535034\n",
      "epoch:598, loss: 3.8496084213256836\n",
      "train:0.6575776934623718, val: 0.4881656765937805\n",
      "epoch:599, loss: 3.849616765975952\n",
      "train:0.667723536491394, val: 0.5059171319007874\n",
      "epoch:600, loss: 3.8496060371398926\n",
      "train:0.6569435596466064, val: 0.4792899489402771\n",
      "epoch:601, loss: 3.8496134281158447\n",
      "train:0.6594800353050232, val: 0.48520711064338684\n",
      "epoch:602, loss: 3.849618434906006\n",
      "train:0.6442612409591675, val: 0.4585798680782318\n",
      "epoch:603, loss: 3.849611759185791\n",
      "train:0.6537730097770691, val: 0.4674556255340576\n",
      "epoch:604, loss: 3.8496062755584717\n",
      "train:0.6626505851745605, val: 0.5133135914802551\n",
      "epoch:605, loss: 3.8496103286743164\n",
      "train:0.6626505851745605, val: 0.4911242723464966\n",
      "epoch:606, loss: 3.849605083465576\n",
      "train:0.6544070839881897, val: 0.4911242723464966\n",
      "epoch:607, loss: 3.849614381790161\n",
      "train:0.6632847189903259, val: 0.4955621361732483\n",
      "epoch:608, loss: 3.8496105670928955\n",
      "train:0.6423588991165161, val: 0.49408283829689026\n",
      "epoch:609, loss: 3.849612236022949\n",
      "train:0.6575776934623718, val: 0.49408283829689026\n",
      "epoch:610, loss: 3.8496081829071045\n",
      "train:0.6607482433319092, val: 0.5177514553070068\n",
      "epoch:611, loss: 3.849614381790161\n",
      "train:0.6544070839881897, val: 0.5147929191589355\n",
      "epoch:612, loss: 3.849606990814209\n",
      "train:0.667723536491394, val: 0.5103550553321838\n",
      "epoch:613, loss: 3.849609375\n",
      "train:0.6607482433319092, val: 0.5014792680740356\n",
      "epoch:614, loss: 3.8496084213256836\n",
      "train:0.6550412178039551, val: 0.49408283829689026\n",
      "epoch:615, loss: 3.849609375\n",
      "train:0.6487000584602356, val: 0.48520711064338684\n",
      "epoch:616, loss: 3.849611520767212\n",
      "train:0.6341153979301453, val: 0.4926035404205322\n",
      "epoch:617, loss: 3.8496134281158447\n",
      "train:0.6398224234580994, val: 0.5029585957527161\n",
      "epoch:618, loss: 3.8496148586273193\n",
      "train:0.6436271667480469, val: 0.48964497447013855\n",
      "epoch:619, loss: 3.849609375\n",
      "train:0.6518706679344177, val: 0.5177514553070068\n",
      "epoch:620, loss: 3.849604368209839\n",
      "train:0.6556753516197205, val: 0.5044378638267517\n",
      "epoch:621, loss: 3.8496127128601074\n",
      "train:0.650602400302887, val: 0.48964497447013855\n",
      "epoch:622, loss: 3.8496038913726807\n",
      "train:0.6651870608329773, val: 0.48076921701431274\n",
      "epoch:623, loss: 3.8496062755584717\n",
      "train:0.6620165109634399, val: 0.5073964595794678\n",
      "epoch:624, loss: 3.8496105670928955\n",
      "train:0.6607482433319092, val: 0.4955621361732483\n",
      "epoch:625, loss: 3.849611282348633\n",
      "train:0.6746987700462341, val: 0.47781065106391907\n",
      "epoch:626, loss: 3.849611759185791\n",
      "train:0.6499682664871216, val: 0.47633135318756104\n",
      "epoch:627, loss: 3.8496124744415283\n",
      "train:0.6708940863609314, val: 0.49408283829689026\n",
      "epoch:628, loss: 3.849600315093994\n",
      "train:0.6727964282035828, val: 0.5073964595794678\n",
      "epoch:629, loss: 3.8496079444885254\n",
      "train:0.6645529270172119, val: 0.4911242723464966\n",
      "epoch:630, loss: 3.8496081829071045\n",
      "train:0.6556753516197205, val: 0.5162721872329712\n",
      "epoch:631, loss: 3.8496031761169434\n",
      "train:0.6569435596466064, val: 0.4837278127670288\n",
      "epoch:632, loss: 3.8496041297912598\n",
      "train:0.6563094258308411, val: 0.5428994297981262\n",
      "epoch:633, loss: 3.8496060371398926\n",
      "train:0.6398224234580994, val: 0.4970414340496063\n",
      "epoch:634, loss: 3.8496012687683105\n",
      "train:0.6448953747749329, val: 0.47781065106391907\n",
      "epoch:635, loss: 3.849612236022949\n",
      "train:0.6467977166175842, val: 0.4837278127670288\n",
      "epoch:636, loss: 3.8496084213256836\n",
      "train:0.6537730097770691, val: 0.5221893787384033\n",
      "epoch:637, loss: 3.849613666534424\n",
      "train:0.6442612409591675, val: 0.4837278127670288\n",
      "epoch:638, loss: 3.8496134281158447\n",
      "train:0.6632847189903259, val: 0.49852070212364197\n",
      "epoch:639, loss: 3.849602222442627\n",
      "train:0.6429930329322815, val: 0.4837278127670288\n",
      "epoch:640, loss: 3.849613904953003\n",
      "train:0.6639188528060913, val: 0.5103550553321838\n",
      "epoch:641, loss: 3.8496124744415283\n",
      "train:0.6429930329322815, val: 0.4926035404205322\n",
      "epoch:642, loss: 3.8496127128601074\n",
      "train:0.6328471899032593, val: 0.5014792680740356\n",
      "epoch:643, loss: 3.849606990814209\n",
      "train:0.636651873588562, val: 0.47337278723716736\n",
      "epoch:644, loss: 3.8496060371398926\n",
      "train:0.6651870608329773, val: 0.4926035404205322\n",
      "epoch:645, loss: 3.8496153354644775\n",
      "train:0.6391883492469788, val: 0.5162721872329712\n",
      "epoch:646, loss: 3.8496153354644775\n",
      "train:0.6588459014892578, val: 0.5\n",
      "epoch:647, loss: 3.8496031761169434\n",
      "train:0.6474318504333496, val: 0.47781065106391907\n",
      "epoch:648, loss: 3.8496127128601074\n",
      "train:0.6759670376777649, val: 0.4955621361732483\n",
      "epoch:649, loss: 3.8496170043945312\n",
      "train:0.6512365341186523, val: 0.4926035404205322\n",
      "epoch:650, loss: 3.849604845046997\n",
      "train:0.6588459014892578, val: 0.4866863787174225\n",
      "epoch:651, loss: 3.849618911743164\n",
      "train:0.6544070839881897, val: 0.49852070212364197\n",
      "epoch:652, loss: 3.849618673324585\n",
      "train:0.6467977166175842, val: 0.4955621361732483\n",
      "epoch:653, loss: 3.849607229232788\n",
      "train:0.6664552688598633, val: 0.5073964595794678\n",
      "epoch:654, loss: 3.849609375\n",
      "train:0.6334812641143799, val: 0.4955621361732483\n",
      "epoch:655, loss: 3.8496100902557373\n",
      "train:0.6448953747749329, val: 0.5088757276535034\n",
      "epoch:656, loss: 3.849601984024048\n",
      "train:0.6550412178039551, val: 0.5014792680740356\n",
      "epoch:657, loss: 3.849611282348633\n",
      "train:0.6480659246444702, val: 0.4866863787174225\n",
      "epoch:658, loss: 3.8496105670928955\n",
      "train:0.6569435596466064, val: 0.5118343234062195\n",
      "epoch:659, loss: 3.849607467651367\n",
      "train:0.6531388759613037, val: 0.47633135318756104\n",
      "epoch:660, loss: 3.849602222442627\n",
      "train:0.667723536491394, val: 0.4792899489402771\n",
      "epoch:661, loss: 3.8496079444885254\n",
      "train:0.650602400302887, val: 0.49408283829689026\n",
      "epoch:662, loss: 3.8496086597442627\n",
      "train:0.6525047421455383, val: 0.5118343234062195\n",
      "epoch:663, loss: 3.8496062755584717\n",
      "train:0.6499682664871216, val: 0.5162721872329712\n",
      "epoch:664, loss: 3.8496007919311523\n",
      "train:0.6442612409591675, val: 0.4792899489402771\n",
      "epoch:665, loss: 3.8496108055114746\n",
      "train:0.6727964282035828, val: 0.5221893787384033\n",
      "epoch:666, loss: 3.8496055603027344\n",
      "train:0.6715282201766968, val: 0.5177514553070068\n",
      "epoch:667, loss: 3.849611759185791\n",
      "train:0.6385542154312134, val: 0.48964497447013855\n",
      "epoch:668, loss: 3.849606990814209\n",
      "train:0.6569435596466064, val: 0.48964497447013855\n",
      "epoch:669, loss: 3.8496131896972656\n",
      "train:0.6410906910896301, val: 0.5\n",
      "epoch:670, loss: 3.849602222442627\n",
      "train:0.6651870608329773, val: 0.4955621361732483\n",
      "epoch:671, loss: 3.8496198654174805\n",
      "train:0.6322130560874939, val: 0.4837278127670288\n",
      "epoch:672, loss: 3.849614381790161\n",
      "train:0.6753329038619995, val: 0.4866863787174225\n",
      "epoch:673, loss: 3.8496251106262207\n",
      "train:0.6658211946487427, val: 0.5251479148864746\n",
      "epoch:674, loss: 3.8496127128601074\n",
      "train:0.573240339756012, val: 0.42751479148864746\n",
      "epoch:675, loss: 3.8496577739715576\n",
      "train:0.6398224234580994, val: 0.48964497447013855\n",
      "epoch:676, loss: 3.849623918533325\n",
      "train:0.6461635828018188, val: 0.4792899489402771\n",
      "epoch:677, loss: 3.8496134281158447\n",
      "train:0.635383665561676, val: 0.4955621361732483\n",
      "epoch:678, loss: 3.849602222442627\n",
      "train:0.6544070839881897, val: 0.5029585957527161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:679, loss: 3.849614381790161\n",
      "train:0.6569435596466064, val: 0.47633135318756104\n",
      "epoch:680, loss: 3.8496170043945312\n",
      "train:0.66899174451828, val: 0.4881656765937805\n",
      "epoch:681, loss: 3.8496124744415283\n",
      "train:0.6569435596466064, val: 0.48076921701431274\n",
      "epoch:682, loss: 3.849607467651367\n",
      "train:0.6563094258308411, val: 0.5044378638267517\n",
      "epoch:683, loss: 3.8495967388153076\n",
      "train:0.6487000584602356, val: 0.5014792680740356\n",
      "epoch:684, loss: 3.849607467651367\n",
      "train:0.6423588991165161, val: 0.48964497447013855\n",
      "epoch:685, loss: 3.849609375\n",
      "train:0.6575776934623718, val: 0.4718934893608093\n",
      "epoch:686, loss: 3.849606990814209\n",
      "train:0.6582117676734924, val: 0.4792899489402771\n",
      "epoch:687, loss: 3.849606513977051\n",
      "train:0.6512365341186523, val: 0.4911242723464966\n",
      "epoch:688, loss: 3.849601984024048\n",
      "train:0.6455295085906982, val: 0.5044378638267517\n",
      "epoch:689, loss: 3.849600315093994\n",
      "train:0.6588459014892578, val: 0.4748520851135254\n",
      "epoch:690, loss: 3.849601984024048\n",
      "train:0.6626505851745605, val: 0.4911242723464966\n",
      "epoch:691, loss: 3.8496217727661133\n",
      "train:0.6467977166175842, val: 0.4911242723464966\n",
      "epoch:692, loss: 3.849609375\n",
      "train:0.6512365341186523, val: 0.4718934893608093\n",
      "epoch:693, loss: 3.849607467651367\n",
      "train:0.6594800353050232, val: 0.48520711064338684\n",
      "epoch:694, loss: 3.849606513977051\n",
      "train:0.650602400302887, val: 0.5044378638267517\n",
      "epoch:695, loss: 3.8496034145355225\n",
      "train:0.6467977166175842, val: 0.5029585957527161\n",
      "epoch:696, loss: 3.8496108055114746\n",
      "train:0.6531388759613037, val: 0.4822485148906708\n",
      "epoch:697, loss: 3.8496031761169434\n",
      "train:0.6544070839881897, val: 0.48964497447013855\n",
      "epoch:698, loss: 3.849609613418579\n",
      "train:0.6512365341186523, val: 0.5014792680740356\n",
      "epoch:699, loss: 3.849602222442627\n",
      "train:0.6544070839881897, val: 0.5192307829856873\n",
      "epoch:700, loss: 3.849600076675415\n",
      "train:0.6563094258308411, val: 0.5014792680740356\n",
      "epoch:701, loss: 3.8496103286743164\n",
      "train:0.6309448480606079, val: 0.4970414340496063\n",
      "epoch:702, loss: 3.8496081829071045\n",
      "train:0.6448953747749329, val: 0.4911242723464966\n",
      "epoch:703, loss: 3.8496108055114746\n",
      "train:0.6556753516197205, val: 0.5\n",
      "epoch:704, loss: 3.849601984024048\n",
      "train:0.667723536491394, val: 0.4955621361732483\n",
      "epoch:705, loss: 3.8496108055114746\n",
      "train:0.6544070839881897, val: 0.47781065106391907\n",
      "epoch:706, loss: 3.8496155738830566\n",
      "train:0.6601141691207886, val: 0.4970414340496063\n",
      "epoch:707, loss: 3.849595546722412\n",
      "train:0.6499682664871216, val: 0.47781065106391907\n",
      "epoch:708, loss: 3.849607467651367\n",
      "train:0.6620165109634399, val: 0.46449702978134155\n",
      "epoch:709, loss: 3.8496062755584717\n",
      "train:0.6531388759613037, val: 0.4718934893608093\n",
      "epoch:710, loss: 3.8496041297912598\n",
      "train:0.6594800353050232, val: 0.5\n",
      "epoch:711, loss: 3.8496007919311523\n",
      "train:0.6639188528060913, val: 0.5044378638267517\n",
      "epoch:712, loss: 3.849605083465576\n",
      "train:0.6588459014892578, val: 0.4822485148906708\n",
      "epoch:713, loss: 3.849609375\n",
      "train:0.6341153979301453, val: 0.48520711064338684\n",
      "epoch:714, loss: 3.849611282348633\n",
      "train:0.6512365341186523, val: 0.5088757276535034\n",
      "epoch:715, loss: 3.8496081829071045\n",
      "train:0.6556753516197205, val: 0.48076921701431274\n",
      "epoch:716, loss: 3.849609613418579\n",
      "train:0.6442612409591675, val: 0.5\n",
      "epoch:717, loss: 3.849604368209839\n",
      "train:0.6487000584602356, val: 0.4718934893608093\n",
      "epoch:718, loss: 3.8496131896972656\n",
      "train:0.6575776934623718, val: 0.5014792680740356\n",
      "epoch:719, loss: 3.849595546722412\n",
      "train:0.6391883492469788, val: 0.47781065106391907\n",
      "epoch:720, loss: 3.849609375\n",
      "train:0.6442612409591675, val: 0.5118343234062195\n",
      "epoch:721, loss: 3.849611759185791\n",
      "train:0.6582117676734924, val: 0.5059171319007874\n",
      "epoch:722, loss: 3.8496131896972656\n",
      "train:0.6575776934623718, val: 0.47337278723716736\n",
      "epoch:723, loss: 3.849599838256836\n",
      "train:0.6569435596466064, val: 0.4718934893608093\n",
      "epoch:724, loss: 3.8496100902557373\n",
      "train:0.6658211946487427, val: 0.5088757276535034\n",
      "epoch:725, loss: 3.8496108055114746\n",
      "train:0.6404565572738647, val: 0.48520711064338684\n",
      "epoch:726, loss: 3.849613904953003\n",
      "train:0.6708940863609314, val: 0.4822485148906708\n",
      "epoch:727, loss: 3.8496086597442627\n",
      "train:0.637920081615448, val: 0.48520711064338684\n",
      "epoch:728, loss: 3.849602460861206\n",
      "train:0.6588459014892578, val: 0.4955621361732483\n",
      "epoch:729, loss: 3.849606990814209\n",
      "train:0.6556753516197205, val: 0.47633135318756104\n",
      "epoch:730, loss: 3.8496198654174805\n",
      "train:0.6448953747749329, val: 0.49852070212364197\n",
      "epoch:731, loss: 3.849604368209839\n",
      "train:0.6461635828018188, val: 0.4970414340496063\n",
      "epoch:732, loss: 3.849609375\n",
      "train:0.6594800353050232, val: 0.46893492341041565\n",
      "epoch:733, loss: 3.8496108055114746\n",
      "train:0.6594800353050232, val: 0.5073964595794678\n",
      "epoch:734, loss: 3.8496034145355225\n",
      "train:0.6575776934623718, val: 0.5088757276535034\n",
      "epoch:735, loss: 3.8496084213256836\n",
      "train:0.6632847189903259, val: 0.4792899489402771\n",
      "epoch:736, loss: 3.849597930908203\n",
      "train:0.6702600121498108, val: 0.48520711064338684\n",
      "epoch:737, loss: 3.8496148586273193\n",
      "train:0.6550412178039551, val: 0.4911242723464966\n",
      "epoch:738, loss: 3.8496060371398926\n",
      "train:0.6582117676734924, val: 0.4926035404205322\n",
      "epoch:739, loss: 3.8496105670928955\n",
      "train:0.6448953747749329, val: 0.5162721872329712\n",
      "epoch:740, loss: 3.8496041297912598\n",
      "train:0.6658211946487427, val: 0.49408283829689026\n",
      "epoch:741, loss: 3.8496060371398926\n",
      "train:0.6664552688598633, val: 0.4881656765937805\n",
      "epoch:742, loss: 3.849611520767212\n",
      "train:0.6810399293899536, val: 0.5088757276535034\n",
      "epoch:743, loss: 3.8496134281158447\n",
      "train:0.6588459014892578, val: 0.4955621361732483\n",
      "epoch:744, loss: 3.8496134281158447\n",
      "train:0.6607482433319092, val: 0.4748520851135254\n",
      "epoch:745, loss: 3.8496146202087402\n",
      "train:0.6651870608329773, val: 0.4748520851135254\n",
      "epoch:746, loss: 3.849605083465576\n",
      "train:0.6544070839881897, val: 0.5103550553321838\n",
      "epoch:747, loss: 3.849611759185791\n",
      "train:0.6613823771476746, val: 0.48964497447013855\n",
      "epoch:748, loss: 3.849612236022949\n",
      "train:0.6398224234580994, val: 0.4970414340496063\n",
      "epoch:749, loss: 3.8496053218841553\n",
      "train:0.6448953747749329, val: 0.48964497447013855\n",
      "epoch:750, loss: 3.8496108055114746\n",
      "train:0.6474318504333496, val: 0.5103550553321838\n",
      "epoch:751, loss: 3.8496108055114746\n",
      "train:0.6544070839881897, val: 0.48964497447013855\n",
      "epoch:752, loss: 3.8496105670928955\n",
      "train:0.6417247653007507, val: 0.48964497447013855\n",
      "epoch:753, loss: 3.849614381790161\n",
      "train:0.6613823771476746, val: 0.47781065106391907\n",
      "epoch:754, loss: 3.8496079444885254\n",
      "train:0.6423588991165161, val: 0.4674556255340576\n",
      "epoch:755, loss: 3.8496108055114746\n",
      "train:0.6626505851745605, val: 0.5014792680740356\n",
      "epoch:756, loss: 3.8496081829071045\n",
      "train:0.6563094258308411, val: 0.4822485148906708\n",
      "epoch:757, loss: 3.8496017456054688\n",
      "train:0.6290425062179565, val: 0.4970414340496063\n",
      "epoch:758, loss: 3.8496055603027344\n",
      "train:0.6417247653007507, val: 0.49852070212364197\n",
      "epoch:759, loss: 3.849611759185791\n",
      "train:0.6429930329322815, val: 0.5118343234062195\n",
      "epoch:760, loss: 3.8496012687683105\n",
      "train:0.6467977166175842, val: 0.48076921701431274\n",
      "epoch:761, loss: 3.849606513977051\n",
      "train:0.6651870608329773, val: 0.47337278723716736\n",
      "epoch:762, loss: 3.8496124744415283\n",
      "train:0.6550412178039551, val: 0.4881656765937805\n",
      "epoch:763, loss: 3.8496155738830566\n",
      "train:0.6683576703071594, val: 0.49852070212364197\n",
      "epoch:764, loss: 3.8496012687683105\n",
      "train:0.6658211946487427, val: 0.5014792680740356\n",
      "epoch:765, loss: 3.849602460861206\n",
      "train:0.6550412178039551, val: 0.4674556255340576\n",
      "epoch:766, loss: 3.849612236022949\n",
      "train:0.650602400302887, val: 0.47781065106391907\n",
      "epoch:767, loss: 3.849607467651367\n",
      "train:0.6544070839881897, val: 0.47337278723716736\n",
      "epoch:768, loss: 3.8496029376983643\n",
      "train:0.6721623539924622, val: 0.4659763276576996\n",
      "epoch:769, loss: 3.849609375\n",
      "train:0.6537730097770691, val: 0.4955621361732483\n",
      "epoch:770, loss: 3.8496100902557373\n",
      "train:0.6645529270172119, val: 0.48964497447013855\n",
      "epoch:771, loss: 3.8496034145355225\n",
      "train:0.6556753516197205, val: 0.4955621361732483\n",
      "epoch:772, loss: 3.8496148586273193\n",
      "train:0.6487000584602356, val: 0.5133135914802551\n",
      "epoch:773, loss: 3.8496079444885254\n",
      "train:0.6651870608329773, val: 0.4911242723464966\n",
      "epoch:774, loss: 3.8496100902557373\n",
      "train:0.6455295085906982, val: 0.5177514553070068\n",
      "epoch:775, loss: 3.849597930908203\n",
      "train:0.6474318504333496, val: 0.5\n",
      "epoch:776, loss: 3.849606990814209\n",
      "train:0.6753329038619995, val: 0.49852070212364197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:777, loss: 3.849606513977051\n",
      "train:0.6550412178039551, val: 0.5\n",
      "epoch:778, loss: 3.849601984024048\n",
      "train:0.6423588991165161, val: 0.4659763276576996\n",
      "epoch:779, loss: 3.8496193885803223\n",
      "train:0.6315789222717285, val: 0.49408283829689026\n",
      "epoch:780, loss: 3.8496475219726562\n",
      "train:0.6512365341186523, val: 0.45710060000419617\n",
      "epoch:781, loss: 3.8496251106262207\n",
      "train:0.6607482433319092, val: 0.47781065106391907\n",
      "epoch:782, loss: 3.849637031555176\n",
      "train:0.6467977166175842, val: 0.5384615659713745\n",
      "epoch:783, loss: 3.849606513977051\n",
      "train:0.6487000584602356, val: 0.4866863787174225\n",
      "epoch:784, loss: 3.849606513977051\n",
      "train:0.6461635828018188, val: 0.5133135914802551\n",
      "epoch:785, loss: 3.8496196269989014\n",
      "train:0.5415345430374146, val: 0.4245562255382538\n",
      "epoch:786, loss: 3.8496901988983154\n",
      "train:0.6645529270172119, val: 0.47781065106391907\n",
      "epoch:787, loss: 3.8496146202087402\n",
      "train:0.6233354210853577, val: 0.46893492341041565\n",
      "epoch:788, loss: 3.8496546745300293\n",
      "train:0.6467977166175842, val: 0.48520711064338684\n",
      "epoch:789, loss: 3.849622964859009\n",
      "train:0.5833861827850342, val: 0.4911242723464966\n",
      "epoch:790, loss: 3.8496453762054443\n",
      "train:0.6575776934623718, val: 0.4911242723464966\n",
      "epoch:791, loss: 3.8496146202087402\n",
      "train:0.6512365341186523, val: 0.5044378638267517\n",
      "epoch:792, loss: 3.8496131896972656\n",
      "train:0.6645529270172119, val: 0.5029585957527161\n",
      "epoch:793, loss: 3.849612236022949\n",
      "train:0.6442612409591675, val: 0.5103550553321838\n",
      "epoch:794, loss: 3.849602460861206\n",
      "train:0.6474318504333496, val: 0.48964497447013855\n",
      "epoch:795, loss: 3.849607467651367\n",
      "train:0.6328471899032593, val: 0.4970414340496063\n",
      "epoch:796, loss: 3.849613666534424\n",
      "train:0.6664552688598633, val: 0.4837278127670288\n",
      "epoch:797, loss: 3.849609613418579\n",
      "train:0.6658211946487427, val: 0.4911242723464966\n",
      "epoch:798, loss: 3.849611520767212\n",
      "train:0.6525047421455383, val: 0.5\n",
      "epoch:799, loss: 3.8496029376983643\n",
      "train:0.667723536491394, val: 0.47781065106391907\n",
      "epoch:800, loss: 3.8496029376983643\n",
      "train:0.6588459014892578, val: 0.48076921701431274\n",
      "epoch:801, loss: 3.849621534347534\n",
      "train:0.6670894026756287, val: 0.49852070212364197\n",
      "epoch:802, loss: 3.8496086597442627\n",
      "train:0.6455295085906982, val: 0.5073964595794678\n",
      "epoch:803, loss: 3.8496170043945312\n",
      "train:0.6626505851745605, val: 0.4955621361732483\n",
      "epoch:804, loss: 3.849604368209839\n",
      "train:0.6601141691207886, val: 0.4792899489402771\n",
      "epoch:805, loss: 3.8496105670928955\n",
      "train:0.6639188528060913, val: 0.47337278723716736\n",
      "epoch:806, loss: 3.8496131896972656\n",
      "train:0.6455295085906982, val: 0.5029585957527161\n",
      "epoch:807, loss: 3.849607467651367\n",
      "train:0.6512365341186523, val: 0.5221893787384033\n",
      "epoch:808, loss: 3.8496108055114746\n",
      "train:0.6582117676734924, val: 0.4955621361732483\n",
      "epoch:809, loss: 3.8496079444885254\n",
      "train:0.6607482433319092, val: 0.4926035404205322\n",
      "epoch:810, loss: 3.849604845046997\n",
      "train:0.6607482433319092, val: 0.48520711064338684\n",
      "epoch:811, loss: 3.8496041297912598\n",
      "train:0.6601141691207886, val: 0.4866863787174225\n",
      "epoch:812, loss: 3.8496041297912598\n",
      "train:0.6569435596466064, val: 0.48076921701431274\n",
      "epoch:813, loss: 3.849602460861206\n",
      "train:0.667723536491394, val: 0.4926035404205322\n",
      "epoch:814, loss: 3.8496105670928955\n",
      "train:0.6550412178039551, val: 0.4970414340496063\n",
      "epoch:815, loss: 3.849601984024048\n",
      "train:0.6474318504333496, val: 0.5014792680740356\n",
      "epoch:816, loss: 3.8496086597442627\n",
      "train:0.6601141691207886, val: 0.48076921701431274\n",
      "epoch:817, loss: 3.8496170043945312\n",
      "train:0.66899174451828, val: 0.5014792680740356\n",
      "epoch:818, loss: 3.849607229232788\n",
      "train:0.6455295085906982, val: 0.47781065106391907\n",
      "epoch:819, loss: 3.849609136581421\n",
      "train:0.6569435596466064, val: 0.5295857787132263\n",
      "epoch:820, loss: 3.8496081829071045\n",
      "train:0.649334192276001, val: 0.5014792680740356\n",
      "epoch:821, loss: 3.849607229232788\n",
      "train:0.6658211946487427, val: 0.4792899489402771\n",
      "epoch:822, loss: 3.849602460861206\n",
      "train:0.6715282201766968, val: 0.48964497447013855\n",
      "epoch:823, loss: 3.8496012687683105\n",
      "train:0.6518706679344177, val: 0.5\n",
      "epoch:824, loss: 3.849611520767212\n",
      "train:0.6512365341186523, val: 0.48076921701431274\n",
      "epoch:825, loss: 3.8496108055114746\n",
      "train:0.6461635828018188, val: 0.49852070212364197\n",
      "epoch:826, loss: 3.8495991230010986\n",
      "train:0.6429930329322815, val: 0.47633135318756104\n",
      "epoch:827, loss: 3.8496079444885254\n",
      "train:0.6341153979301453, val: 0.5029585957527161\n",
      "epoch:828, loss: 3.8496031761169434\n",
      "train:0.6429930329322815, val: 0.5029585957527161\n",
      "epoch:829, loss: 3.8496155738830566\n",
      "train:0.6588459014892578, val: 0.4881656765937805\n",
      "epoch:830, loss: 3.8496124744415283\n",
      "train:0.6563094258308411, val: 0.5103550553321838\n",
      "epoch:831, loss: 3.8496034145355225\n",
      "train:0.6639188528060913, val: 0.4970414340496063\n",
      "epoch:832, loss: 3.8496010303497314\n",
      "train:0.6398224234580994, val: 0.5\n",
      "epoch:833, loss: 3.849618434906006\n",
      "train:0.6480659246444702, val: 0.47633135318756104\n",
      "epoch:834, loss: 3.849613904953003\n",
      "train:0.6721623539924622, val: 0.5014792680740356\n",
      "epoch:835, loss: 3.849613666534424\n",
      "train:0.6632847189903259, val: 0.48964497447013855\n",
      "epoch:836, loss: 3.8496108055114746\n",
      "train:0.6626505851745605, val: 0.49408283829689026\n",
      "epoch:837, loss: 3.8496103286743164\n",
      "train:0.6544070839881897, val: 0.5014792680740356\n",
      "epoch:838, loss: 3.849607467651367\n",
      "train:0.649334192276001, val: 0.49852070212364197\n",
      "epoch:839, loss: 3.849606513977051\n",
      "train:0.6537730097770691, val: 0.5103550553321838\n",
      "epoch:840, loss: 3.8496086597442627\n",
      "train:0.6645529270172119, val: 0.5073964595794678\n",
      "epoch:841, loss: 3.8496086597442627\n",
      "train:0.6487000584602356, val: 0.4837278127670288\n",
      "epoch:842, loss: 3.8496177196502686\n",
      "train:0.6499682664871216, val: 0.4970414340496063\n",
      "epoch:843, loss: 3.8496084213256836\n",
      "train:0.6708940863609314, val: 0.4630177617073059\n",
      "epoch:844, loss: 3.8496100902557373\n",
      "train:0.6607482433319092, val: 0.47781065106391907\n",
      "epoch:845, loss: 3.8496081829071045\n",
      "train:0.6702600121498108, val: 0.48964497447013855\n",
      "epoch:846, loss: 3.8496034145355225\n",
      "train:0.6620165109634399, val: 0.48964497447013855\n",
      "epoch:847, loss: 3.849606990814209\n",
      "train:0.6448953747749329, val: 0.4630177617073059\n",
      "epoch:848, loss: 3.849606990814209\n",
      "train:0.6601141691207886, val: 0.5\n",
      "epoch:849, loss: 3.8496103286743164\n",
      "train:0.6544070839881897, val: 0.48076921701431274\n",
      "epoch:850, loss: 3.849613666534424\n",
      "train:0.6639188528060913, val: 0.5207100510597229\n",
      "epoch:851, loss: 3.8496062755584717\n",
      "train:0.6525047421455383, val: 0.4866863787174225\n",
      "epoch:852, loss: 3.849614381790161\n",
      "train:0.6645529270172119, val: 0.48964497447013855\n",
      "epoch:853, loss: 3.849609613418579\n",
      "train:0.6759670376777649, val: 0.49852070212364197\n",
      "epoch:854, loss: 3.8496053218841553\n",
      "train:0.6537730097770691, val: 0.5014792680740356\n",
      "epoch:855, loss: 3.8496084213256836\n",
      "train:0.6417247653007507, val: 0.5207100510597229\n",
      "epoch:856, loss: 3.8496134281158447\n",
      "train:0.6398224234580994, val: 0.4866863787174225\n",
      "epoch:857, loss: 3.8496155738830566\n",
      "train:0.6442612409591675, val: 0.4970414340496063\n",
      "epoch:858, loss: 3.849597692489624\n",
      "train:0.6645529270172119, val: 0.49408283829689026\n",
      "epoch:859, loss: 3.8496079444885254\n",
      "train:0.6658211946487427, val: 0.48076921701431274\n",
      "epoch:860, loss: 3.8496007919311523\n",
      "train:0.6708940863609314, val: 0.5103550553321838\n",
      "epoch:861, loss: 3.8495986461639404\n",
      "train:0.6442612409591675, val: 0.4585798680782318\n",
      "epoch:862, loss: 3.849602222442627\n",
      "train:0.6746987700462341, val: 0.49852070212364197\n",
      "epoch:863, loss: 3.8496131896972656\n",
      "train:0.6461635828018188, val: 0.4792899489402771\n",
      "epoch:864, loss: 3.8496103286743164\n",
      "train:0.6499682664871216, val: 0.48076921701431274\n",
      "epoch:865, loss: 3.849609136581421\n",
      "train:0.6544070839881897, val: 0.4837278127670288\n",
      "epoch:866, loss: 3.8496038913726807\n",
      "train:0.6556753516197205, val: 0.4837278127670288\n",
      "epoch:867, loss: 3.849605083465576\n",
      "train:0.650602400302887, val: 0.5\n",
      "epoch:868, loss: 3.849604845046997\n",
      "train:0.6537730097770691, val: 0.4911242723464966\n",
      "epoch:869, loss: 3.849607229232788\n",
      "train:0.6727964282035828, val: 0.4955621361732483\n",
      "epoch:870, loss: 3.8496155738830566\n",
      "train:0.6550412178039551, val: 0.4837278127670288\n",
      "epoch:871, loss: 3.8496131896972656\n",
      "train:0.6423588991165161, val: 0.4792899489402771\n",
      "epoch:872, loss: 3.849611759185791\n",
      "train:0.650602400302887, val: 0.5118343234062195\n",
      "epoch:873, loss: 3.849611282348633\n",
      "train:0.6632847189903259, val: 0.4955621361732483\n",
      "epoch:874, loss: 3.8496124744415283\n",
      "train:0.6423588991165161, val: 0.49408283829689026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:875, loss: 3.849607229232788\n",
      "train:0.6550412178039551, val: 0.4822485148906708\n",
      "epoch:876, loss: 3.849604845046997\n",
      "train:0.6601141691207886, val: 0.48520711064338684\n",
      "epoch:877, loss: 3.849613666534424\n",
      "train:0.6347495317459106, val: 0.48520711064338684\n",
      "epoch:878, loss: 3.849607229232788\n",
      "train:0.650602400302887, val: 0.48964497447013855\n",
      "epoch:879, loss: 3.849611282348633\n",
      "train:0.6480659246444702, val: 0.4911242723464966\n",
      "epoch:880, loss: 3.849612236022949\n",
      "train:0.6537730097770691, val: 0.4837278127670288\n",
      "epoch:881, loss: 3.8496084213256836\n",
      "train:0.6670894026756287, val: 0.49408283829689026\n",
      "epoch:882, loss: 3.849606513977051\n",
      "train:0.6658211946487427, val: 0.5147929191589355\n",
      "epoch:883, loss: 3.849602460861206\n",
      "train:0.6398224234580994, val: 0.47781065106391907\n",
      "epoch:884, loss: 3.849611520767212\n",
      "train:0.6544070839881897, val: 0.47337278723716736\n",
      "epoch:885, loss: 3.8496041297912598\n",
      "train:0.6626505851745605, val: 0.4866863787174225\n",
      "epoch:886, loss: 3.849609136581421\n",
      "train:0.6664552688598633, val: 0.4970414340496063\n",
      "epoch:887, loss: 3.8496081829071045\n",
      "train:0.6626505851745605, val: 0.4911242723464966\n",
      "epoch:888, loss: 3.8496041297912598\n",
      "train:0.6550412178039551, val: 0.4911242723464966\n",
      "epoch:889, loss: 3.8496012687683105\n",
      "train:0.6613823771476746, val: 0.48076921701431274\n",
      "epoch:890, loss: 3.8496053218841553\n",
      "train:0.6550412178039551, val: 0.4911242723464966\n",
      "epoch:891, loss: 3.849611759185791\n",
      "train:0.66899174451828, val: 0.49408283829689026\n",
      "epoch:892, loss: 3.8496108055114746\n",
      "train:0.6607482433319092, val: 0.4866863787174225\n",
      "epoch:893, loss: 3.849606513977051\n",
      "train:0.6594800353050232, val: 0.49852070212364197\n",
      "epoch:894, loss: 3.8496105670928955\n",
      "train:0.6588459014892578, val: 0.5207100510597229\n",
      "epoch:895, loss: 3.8496062755584717\n",
      "train:0.6696258783340454, val: 0.5221893787384033\n",
      "epoch:896, loss: 3.849600076675415\n",
      "train:0.6544070839881897, val: 0.48520711064338684\n",
      "epoch:897, loss: 3.849614381790161\n",
      "train:0.6639188528060913, val: 0.5044378638267517\n",
      "epoch:898, loss: 3.8496055603027344\n",
      "train:0.6575776934623718, val: 0.47633135318756104\n",
      "epoch:899, loss: 3.849606990814209\n",
      "train:0.6588459014892578, val: 0.5059171319007874\n",
      "epoch:900, loss: 3.8496031761169434\n",
      "train:0.6670894026756287, val: 0.5059171319007874\n",
      "epoch:901, loss: 3.8496086597442627\n",
      "train:0.6632847189903259, val: 0.49852070212364197\n",
      "epoch:902, loss: 3.8496158123016357\n",
      "train:0.6556753516197205, val: 0.5044378638267517\n",
      "epoch:903, loss: 3.8496060371398926\n",
      "train:0.6518706679344177, val: 0.4970414340496063\n",
      "epoch:904, loss: 3.8496124744415283\n",
      "train:0.6721623539924622, val: 0.5059171319007874\n",
      "epoch:905, loss: 3.8496034145355225\n",
      "train:0.6607482433319092, val: 0.5118343234062195\n",
      "epoch:906, loss: 3.8496041297912598\n",
      "train:0.6556753516197205, val: 0.48076921701431274\n",
      "epoch:907, loss: 3.849611759185791\n",
      "train:0.6525047421455383, val: 0.4837278127670288\n",
      "epoch:908, loss: 3.8496086597442627\n",
      "train:0.6518706679344177, val: 0.5\n",
      "epoch:909, loss: 3.8496086597442627\n",
      "train:0.6563094258308411, val: 0.45266273617744446\n",
      "epoch:910, loss: 3.8496060371398926\n",
      "train:0.6550412178039551, val: 0.5029585957527161\n",
      "epoch:911, loss: 3.849607229232788\n",
      "train:0.6601141691207886, val: 0.4837278127670288\n",
      "epoch:912, loss: 3.8496105670928955\n",
      "train:0.6512365341186523, val: 0.47781065106391907\n",
      "epoch:913, loss: 3.8496053218841553\n",
      "train:0.6651870608329773, val: 0.49852070212364197\n",
      "epoch:914, loss: 3.8496038913726807\n",
      "train:0.6696258783340454, val: 0.5073964595794678\n",
      "epoch:915, loss: 3.8496060371398926\n",
      "train:0.6626505851745605, val: 0.4955621361732483\n",
      "epoch:916, loss: 3.8496124744415283\n",
      "train:0.6550412178039551, val: 0.4718934893608093\n",
      "epoch:917, loss: 3.849609136581421\n",
      "train:0.6233354210853577, val: 0.4926035404205322\n",
      "epoch:918, loss: 3.849609375\n",
      "train:0.6727964282035828, val: 0.4970414340496063\n",
      "epoch:919, loss: 3.849600076675415\n",
      "train:0.6455295085906982, val: 0.4837278127670288\n",
      "epoch:920, loss: 3.849602222442627\n",
      "train:0.6417247653007507, val: 0.5162721872329712\n",
      "epoch:921, loss: 3.8496084213256836\n",
      "train:0.6455295085906982, val: 0.523668646812439\n",
      "epoch:922, loss: 3.8496012687683105\n",
      "train:0.6480659246444702, val: 0.48520711064338684\n",
      "epoch:923, loss: 3.8496060371398926\n",
      "train:0.6797717213630676, val: 0.4792899489402771\n",
      "epoch:924, loss: 3.8496127128601074\n",
      "train:0.6664552688598633, val: 0.5059171319007874\n",
      "epoch:925, loss: 3.849606513977051\n",
      "train:0.6632847189903259, val: 0.49852070212364197\n",
      "epoch:926, loss: 3.849602222442627\n",
      "train:0.6537730097770691, val: 0.5103550553321838\n",
      "epoch:927, loss: 3.849609136581421\n",
      "train:0.6461635828018188, val: 0.5118343234062195\n",
      "epoch:928, loss: 3.8496055603027344\n",
      "train:0.667723536491394, val: 0.4866863787174225\n",
      "epoch:929, loss: 3.8496148586273193\n",
      "train:0.6734305620193481, val: 0.4659763276576996\n",
      "epoch:930, loss: 3.8496086597442627\n",
      "train:0.6467977166175842, val: 0.4955621361732483\n",
      "epoch:931, loss: 3.8496124744415283\n",
      "train:0.6740646958351135, val: 0.4911242723464966\n",
      "epoch:932, loss: 3.8496010303497314\n",
      "train:0.6525047421455383, val: 0.4955621361732483\n",
      "epoch:933, loss: 3.8496060371398926\n",
      "train:0.6753329038619995, val: 0.4718934893608093\n",
      "epoch:934, loss: 3.849602222442627\n",
      "train:0.6670894026756287, val: 0.4926035404205322\n",
      "epoch:935, loss: 3.849607229232788\n",
      "train:0.6525047421455383, val: 0.48964497447013855\n",
      "epoch:936, loss: 3.8495981693267822\n",
      "train:0.6537730097770691, val: 0.4881656765937805\n",
      "epoch:937, loss: 3.849607467651367\n",
      "train:0.6512365341186523, val: 0.5177514553070068\n",
      "epoch:938, loss: 3.849614381790161\n",
      "train:0.6664552688598633, val: 0.4955621361732483\n",
      "epoch:939, loss: 3.8495969772338867\n",
      "train:0.6391883492469788, val: 0.48964497447013855\n",
      "epoch:940, loss: 3.849601984024048\n",
      "train:0.6601141691207886, val: 0.4659763276576996\n",
      "epoch:941, loss: 3.8496124744415283\n",
      "train:0.6734305620193481, val: 0.47633135318756104\n",
      "epoch:942, loss: 3.849605083465576\n",
      "train:0.6563094258308411, val: 0.4822485148906708\n",
      "epoch:943, loss: 3.8496100902557373\n",
      "train:0.6727964282035828, val: 0.4659763276576996\n",
      "epoch:944, loss: 3.849607467651367\n",
      "train:0.6322130560874939, val: 0.48964497447013855\n",
      "epoch:945, loss: 3.849600315093994\n",
      "train:0.6525047421455383, val: 0.4955621361732483\n",
      "epoch:946, loss: 3.8496017456054688\n",
      "train:0.6442612409591675, val: 0.5014792680740356\n",
      "epoch:947, loss: 3.8496103286743164\n",
      "train:0.6550412178039551, val: 0.5014792680740356\n",
      "epoch:948, loss: 3.8496251106262207\n",
      "train:0.6404565572738647, val: 0.47781065106391907\n",
      "epoch:949, loss: 3.8496267795562744\n",
      "train:0.667723536491394, val: 0.4881656765937805\n",
      "epoch:950, loss: 3.8496103286743164\n",
      "train:0.6651870608329773, val: 0.5088757276535034\n",
      "epoch:951, loss: 3.8495993614196777\n",
      "train:0.6512365341186523, val: 0.47781065106391907\n",
      "epoch:952, loss: 3.849602460861206\n",
      "train:0.6385542154312134, val: 0.5177514553070068\n",
      "epoch:953, loss: 3.8496081829071045\n",
      "train:0.6563094258308411, val: 0.47337278723716736\n",
      "epoch:954, loss: 3.8496100902557373\n",
      "train:0.6467977166175842, val: 0.49408283829689026\n",
      "epoch:955, loss: 3.8496134281158447\n",
      "train:0.6461635828018188, val: 0.4615384638309479\n",
      "epoch:956, loss: 3.8495945930480957\n",
      "train:0.6569435596466064, val: 0.5\n",
      "epoch:957, loss: 3.849613666534424\n",
      "train:0.6632847189903259, val: 0.48964497447013855\n",
      "epoch:958, loss: 3.849611282348633\n",
      "train:0.6429930329322815, val: 0.5073964595794678\n",
      "epoch:959, loss: 3.8496060371398926\n",
      "train:0.6594800353050232, val: 0.46893492341041565\n",
      "epoch:960, loss: 3.8496055603027344\n",
      "train:0.667723536491394, val: 0.4926035404205322\n",
      "epoch:961, loss: 3.8496041297912598\n",
      "train:0.6607482433319092, val: 0.5044378638267517\n",
      "epoch:962, loss: 3.8496060371398926\n",
      "train:0.6550412178039551, val: 0.49852070212364197\n",
      "epoch:963, loss: 3.849600315093994\n",
      "train:0.6518706679344177, val: 0.5014792680740356\n",
      "epoch:964, loss: 3.849606513977051\n",
      "train:0.6544070839881897, val: 0.5147929191589355\n",
      "epoch:965, loss: 3.8496055603027344\n",
      "train:0.6632847189903259, val: 0.4792899489402771\n",
      "epoch:966, loss: 3.849616527557373\n",
      "train:0.6112872362136841, val: 0.4452662765979767\n",
      "epoch:967, loss: 3.8496382236480713\n",
      "train:0.6639188528060913, val: 0.49852070212364197\n",
      "epoch:968, loss: 3.8496148586273193\n",
      "train:0.667723536491394, val: 0.48076921701431274\n",
      "epoch:969, loss: 3.849600076675415\n",
      "train:0.6455295085906982, val: 0.4881656765937805\n",
      "epoch:970, loss: 3.849609375\n",
      "train:0.66899174451828, val: 0.5044378638267517\n",
      "epoch:971, loss: 3.849604845046997\n",
      "train:0.6588459014892578, val: 0.4881656765937805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:972, loss: 3.849614381790161\n",
      "train:0.6620165109634399, val: 0.4926035404205322\n",
      "epoch:973, loss: 3.849609136581421\n",
      "train:0.6778693795204163, val: 0.4748520851135254\n",
      "epoch:974, loss: 3.849614381790161\n",
      "train:0.6467977166175842, val: 0.4911242723464966\n",
      "epoch:975, loss: 3.849606990814209\n",
      "train:0.6721623539924622, val: 0.47633135318756104\n",
      "epoch:976, loss: 3.8496053218841553\n",
      "train:0.6702600121498108, val: 0.4866863787174225\n",
      "epoch:977, loss: 3.8496055603027344\n",
      "train:0.6512365341186523, val: 0.49852070212364197\n",
      "epoch:978, loss: 3.849612236022949\n",
      "train:0.6455295085906982, val: 0.4866863787174225\n",
      "epoch:979, loss: 3.8496079444885254\n",
      "train:0.6632847189903259, val: 0.5088757276535034\n",
      "epoch:980, loss: 3.849612236022949\n",
      "train:0.6645529270172119, val: 0.45562130212783813\n",
      "epoch:981, loss: 3.849611520767212\n",
      "train:0.6632847189903259, val: 0.5059171319007874\n",
      "epoch:982, loss: 3.849602460861206\n",
      "train:0.6632847189903259, val: 0.48964497447013855\n",
      "epoch:983, loss: 3.849609613418579\n",
      "train:0.6670894026756287, val: 0.4881656765937805\n",
      "epoch:984, loss: 3.8496053218841553\n",
      "train:0.6448953747749329, val: 0.4837278127670288\n",
      "epoch:985, loss: 3.849607229232788\n",
      "train:0.6588459014892578, val: 0.4837278127670288\n",
      "epoch:986, loss: 3.849609613418579\n",
      "train:0.6588459014892578, val: 0.5059171319007874\n",
      "epoch:987, loss: 3.8496060371398926\n",
      "train:0.6512365341186523, val: 0.49852070212364197\n",
      "epoch:988, loss: 3.849606513977051\n",
      "train:0.6601141691207886, val: 0.48964497447013855\n",
      "epoch:989, loss: 3.8496127128601074\n",
      "train:0.6582117676734924, val: 0.47337278723716736\n",
      "epoch:990, loss: 3.849606990814209\n",
      "train:0.6518706679344177, val: 0.49852070212364197\n",
      "epoch:991, loss: 3.8496134281158447\n",
      "train:0.6461635828018188, val: 0.47337278723716736\n",
      "epoch:992, loss: 3.849614381790161\n",
      "train:0.6404565572738647, val: 0.4866863787174225\n",
      "epoch:993, loss: 3.8496084213256836\n",
      "train:0.649334192276001, val: 0.4955621361732483\n",
      "epoch:994, loss: 3.8496055603027344\n",
      "train:0.6651870608329773, val: 0.47781065106391907\n",
      "epoch:995, loss: 3.849600315093994\n",
      "train:0.6708940863609314, val: 0.48964497447013855\n",
      "epoch:996, loss: 3.849614381790161\n",
      "train:0.6512365341186523, val: 0.47337278723716736\n",
      "epoch:997, loss: 3.8496055603027344\n",
      "train:0.650602400302887, val: 0.4704141914844513\n",
      "epoch:998, loss: 3.8496148586273193\n",
      "train:0.6645529270172119, val: 0.4866863787174225\n",
      "epoch:999, loss: 3.8496017456054688\n",
      "train:0.6639188528060913, val: 0.4837278127670288\n",
      "epoch:1000, loss: 3.849609136581421\n",
      "train:0.6588459014892578, val: 0.5014792680740356\n"
     ]
    }
   ],
   "source": [
    "losses, all_accuracy = train_models(clf, dataloader, train_dataset, { \n",
    "      \"learning_rate\": 0.001,\n",
    "      \"num_epochs\": 1000,\n",
    "      \"batch_size\": 2000,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# # Example of target with class probabilities\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randn(3, 5).softmax(dim=1)\n",
    "# output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2925,  1.4684,  1.3207,  1.0955,  1.2411],\n",
       "         [ 0.8146,  0.4136,  1.2870, -1.9066, -0.9810],\n",
       "         [-0.2365, -0.3005,  0.2658, -1.4756, -0.0255]], requires_grad=True),\n",
       " tensor([4, 0, 4]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
